"""
ë‰´ìŠ¤ ë¶„ì„ ëª¨ë“ˆ
- ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° GPT ê°ì •ë¶„ì„
- ë ˆë²¨ë³„ ë§ì¶¤í˜• ìš”ì•½
"""

import streamlit as st
import requests
import logging
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
import openai
import urllib.parse
import re
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

# ë ˆë²¨ë³„ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸
LEVEL_PROMPTS = {
    1: """- Level 1 (ì´ˆë³´ì): 
    â€¢ ì–´íˆ¬: ìœ ì¹˜ì›/ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ì•„ì£¼ ì‰¬ìš´ ë§ë¡œ ì„¤ëª…
    â€¢ ë‚´ìš©: íˆ¬ì ê¸°ì´ˆ ê°œë… ìœ„ì£¼, ë³µì¡í•œ ìš©ì–´ëŠ” ë¹„ìœ ì™€ ì˜ˆì‹œë¡œ ëŒ€ì²´
    â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ í•µì‹¬ë§Œ ìš”ì•½""",
    
    2: """- Level 2 (ì…ë¬¸ì): 
    â€¢ ì–´íˆ¬: ì¤‘ê³ ë“±í•™ìƒë„ ì´í•´ ê°€ëŠ¥í•œ ì‰¬ìš´ ë§ë¡œ ì„¤ëª…
    â€¢ ë‚´ìš©: í•µì‹¬ ê°œë…ê³¼ ì´ìœ ë¥¼ í¬í•¨, ê¸°ë³¸ì ì¸ íˆ¬ì ì§€ì‹ ì „ë‹¬
    â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ì„¤ëª…""",
    
    3: """- Level 3 (ì¤‘ê¸‰ì): 
    â€¢ ì–´íˆ¬: ì¼ë°˜ ì„±ì¸ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…
    â€¢ ë‚´ìš©: ì‹¤ì „ íŒê³¼ êµ¬ì²´ì  ì „ëµ í¬í•¨, ë°ì´í„° ê¸°ë°˜ ë¶„ì„
    â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ë¶„ì„""",
    
    4: """- Level 4 (ê³ ê¸‰ì): 
    â€¢ ì–´íˆ¬: íˆ¬ì ê²½í—˜ì´ ìˆëŠ” ì„±ì¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì „ë¬¸ì  ì„¤ëª…
    â€¢ ë‚´ìš©: ì‹¬í™” ë¶„ì„ê³¼ ê³ ê¸‰ ì „ëµ, ì‹œì¥ ë™í–¥ê³¼ ì—°ê´€ì„± ë¶„ì„
    â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ìƒì„¸ ì„¤ëª…""",
    
    5: """- Level 5 (ì „ë¬¸ê°€): 
    â€¢ ì–´íˆ¬: íˆ¬ì ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ê³ ê¸‰ ë¶„ì„ê³¼ ì „ë¬¸ ìš©ì–´ ì‚¬ìš©
    â€¢ ë‚´ìš©: ìµœê³  ìˆ˜ì¤€ ë¶„ì„ê³¼ ì‹¤ì „ í™œìš©, ì‹œì¥ ë¯¸ì‹œêµ¬ì¡°ê¹Œì§€ ê³ ë ¤
    â€¢ ê¸¸ì´: 1-2ì¤„ ì´ìƒìœ¼ë¡œ ì „ë¬¸ì  ì„¤ëª…"""
}

SYSTEM_PROMPT_ANALYSIS = """
- ë‹¹ì‹ ì€ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ê°ì„± ë¶„ì„ê¸°ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ ë³´ë‚¸ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë³´ê³ , ë‹¤ìŒ 3ê°œì˜ í•„ë“œë§Œ "|" ë¡œ êµ¬ë¶„í•´ í•œ ì¤„ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
  1) ë‰´ìŠ¤ê¸°ì‚¬(ì›ë¬¸ ê·¸ëŒ€ë¡œ)
  2) ê¸ë¶€ì • ê²°ê³¼ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜)
  3) ì´ìœ  (í•œ ë¬¸ì¥)
- ë°˜ë“œì‹œ ìœ„ ìˆœì„œëŒ€ë¡œ "ë‰´ìŠ¤ê¸°ì‚¬|ê¸ë¶€ì • ê²°ê³¼|ì´ìœ " í˜•íƒœë¡œë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.
"""

class NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.timeout = 10  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
        self.max_retries = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    
    def _is_valid_url(self, url: str) -> bool:
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            parsed = urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except Exception:
            return False
    
    def _sanitize_url(self, url: str) -> str:
        """URL ì •ê·œí™” ë° ë³´ì•ˆ ê²€ì‚¬"""
        if not url:
            return ""
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if url.startswith('/'):
            url = f"https://search.naver.com{url}"
        elif not url.startswith('http'):
            url = f"https://search.naver.com/{url}"
        
        # URL ìœ íš¨ì„± ê²€ì‚¬
        if not self._is_valid_url(url):
            return ""
        
        # í—ˆìš©ëœ ë„ë©”ì¸ë§Œ í—ˆìš©
        allowed_domains = ['search.naver.com', 'news.naver.com', 'finance.naver.com']
        parsed = urlparse(url)
        if parsed.netloc not in allowed_domains:
            return ""
        
        return url
    
    def fetch_naver_news(self, code: str) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ê³¼ ë§í¬ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not code or not isinstance(code, str):
                logger.warning("ìœ íš¨í•˜ì§€ ì•Šì€ ì½”ë“œ ì…ë ¥")
                return []
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
            if not code.isdigit() and ('ETF' in code.upper() or 'ë°˜ë„ì²´' in code or '2ì°¨ì „ì§€' in code or 'KOSPI' in code.upper()):
                # í‚¤ì›Œë“œë³„ ê´€ë ¨ ê²€ìƒ‰ì–´ ì¶”ê°€
                related_keywords = self._get_related_keywords(code)
                return self._search_naver_news_with_keywords(code, related_keywords)
            
            # ì¢…ëª© ì½”ë“œ ê¸°ë°˜ ê²€ìƒ‰
            stock_name = self._get_stock_name(code)
            if stock_name:
                return self._search_naver_news_simple(stock_name)
            
            # ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            return self._search_naver_news_simple('ì£¼ì‹ íˆ¬ì')
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨ ({code}): {e}")
            return []
    
    def _get_stock_name(self, code: str) -> str:
        """ì¢…ëª© ì½”ë“œë¡œ ì¢…ëª©ëª… ê°€ì ¸ì˜¤ê¸°"""
        stock_names = {
            '091160': 'KBSTAR 200',
            '091170': 'KBSTAR ì½”ìŠ¤ë‹¥150',
            '091230': 'KBSTAR ë°˜ë„ì²´',
            '306540': 'KBSTAR 2ì°¨ì „ì§€í…Œë§ˆ',
            '233740': 'KBSTAR K-ë‰´ë”œë””ì§€í„¸í”ŒëŸ¬ìŠ¤',
            '005930': 'ì‚¼ì„±ì „ì',
            '000660': 'SKí•˜ì´ë‹‰ìŠ¤',
            '035420': 'NAVER',
            '035720': 'ì¹´ì¹´ì˜¤',
            '373220': 'LGì—ë„ˆì§€ì†”ë£¨ì…˜'
        }
        return stock_names.get(code, '')
    
    def _get_keyword_mapping(self) -> Dict[str, List[str]]:
        """í‚¤ì›Œë“œ ë§¤í•‘ ê·œì¹™ ì •ì˜"""
        return {
            # ETF ê´€ë ¨ í‚¤ì›Œë“œ
            'etf': {
                'ë°˜ë„ì²´': ['ë°˜ë„ì²´', 'ë°˜ë„ì²´ì£¼', 'ë°˜ë„ì²´ ETF'],
                '2ì°¨ì „ì§€': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', '2ì°¨ì „ì§€ ETF'],
                'ë°°í„°ë¦¬': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'ë°°í„°ë¦¬ ETF'],
                'kospi': ['KOSPI', 'ì½”ìŠ¤í”¼', 'ëŒ€í˜•ì£¼', 'KOSPI ETF'],
                'kosdaq': ['KOSDAQ', 'ì½”ìŠ¤ë‹¥', 'ì¤‘ì†Œí˜•ì£¼', 'KOSDAQ ETF'],
                'default': ['ì£¼ì‹', 'íˆ¬ì']
            },
            # ì„¹í„°ë³„ í‚¤ì›Œë“œ
            'ë°˜ë„ì²´': ['ë°˜ë„ì²´', 'ë°˜ë„ì²´ì£¼', 'ë°˜ë„ì²´ ì‚°ì—…', 'ë©”ëª¨ë¦¬'],
            '2ì°¨ì „ì§€': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'ì „ê¸°ì°¨ ë°°í„°ë¦¬', 'ë¦¬íŠ¬'],
            'ë°°í„°ë¦¬': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'ì „ê¸°ì°¨ ë°°í„°ë¦¬', 'ë¦¬íŠ¬'],
            'kospi': ['KOSPI', 'ì½”ìŠ¤í”¼', 'ëŒ€í˜•ì£¼', 'ì£¼ì‹ì‹œì¥'],
            'kosdaq': ['KOSDAQ', 'ì½”ìŠ¤ë‹¥', 'ì¤‘ì†Œí˜•ì£¼', 'ê¸°ìˆ ì£¼'],
            'ai': ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹'],
            'ë°”ì´ì˜¤': ['ë°”ì´ì˜¤', 'ì œì•½', 'ì˜ë£Œ', 'í—¬ìŠ¤ì¼€ì–´'],
            'ê²Œì„': ['ê²Œì„', 'ê²Œì„ì£¼', 'ëª¨ë°”ì¼ê²Œì„', 'ì½˜í…ì¸ '],
            'ì „ê¸°ì°¨': ['ì „ê¸°ì°¨', 'EV', 'í…ŒìŠ¬ë¼', 'ì „ê¸°ìë™ì°¨'],
            'ì‹ ì¬ìƒ': ['ì‹ ì¬ìƒ', 'íƒœì–‘ê´‘', 'í’ë ¥', 'ì¹œí™˜ê²½'],
            'ê¸ˆìœµ': ['ê¸ˆìœµ', 'ì€í–‰', 'ë³´í—˜', 'ì¦ê¶Œ'],
            'ë¶€ë™ì‚°': ['ë¶€ë™ì‚°', 'REITs', 'ì•„íŒŒíŠ¸', 'ê±´ì„¤']
        }
    
    def _extract_primary_keyword(self, keyword: str) -> str:
        """í‚¤ì›Œë“œì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keyword_lower = keyword.lower()
        
        # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ í‚¤ì›Œë“œë¶€í„° ë§¤ì¹­
        priority_keywords = [
            'ë°˜ë„ì²´', '2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'kospi', 'kosdaq', 
            'ai', 'ë°”ì´ì˜¤', 'ê²Œì„', 'ì „ê¸°ì°¨', 'ì‹ ì¬ìƒ', 'ê¸ˆìœµ', 'ë¶€ë™ì‚°'
        ]
        
        for primary in priority_keywords:
            if primary in keyword_lower:
                return primary
        
        # ETF í‚¤ì›Œë“œ ì²´í¬
        if 'etf' in keyword_lower:
            return 'etf'
        
        return 'default'
    
    def _get_related_keywords(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œì— ë”°ë¥¸ ê´€ë ¨ ê²€ìƒ‰ì–´ ìƒì„± """
        if not keyword or not isinstance(keyword, str):
            return []
        
        # í‚¤ì›Œë“œ ì •ê·œí™”
        keyword = keyword.strip()
        if not keyword:
            return []
        
        related_keywords = set([keyword])  # ì›ë³¸ í‚¤ì›Œë“œ (setìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€)
        keyword_mapping = self._get_keyword_mapping()
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        primary_keyword = self._extract_primary_keyword(keyword)
        
        # ETF í‚¤ì›Œë“œì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if primary_keyword == 'etf':
            etf_mapping = keyword_mapping.get('etf', {})
            
            # ETF í‚¤ì›Œë“œì—ì„œ ì„¸ë¶€ ì£¼ì œ ì¶”ì¶œ
            keyword_lower = keyword.lower()
            for etf_type, related_list in etf_mapping.items():
                if etf_type != 'default' and etf_type in keyword_lower:
                    related_keywords.update(related_list)
                    break
            else:
                # ê¸°ë³¸ ETF í‚¤ì›Œë“œ
                related_keywords.update(etf_mapping.get('default', ['ì£¼ì‹', 'íˆ¬ì']))
        
        # ì¼ë°˜ ì„¹í„° í‚¤ì›Œë“œ ì²˜ë¦¬
        elif primary_keyword in keyword_mapping:
            related_keywords.update(keyword_mapping[primary_keyword])
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ê°€ (íˆ¬ì ê´€ë ¨)
        if primary_keyword != 'default':
            related_keywords.add('íˆ¬ì')
        
        # í‚¤ì›Œë“œ í’ˆì§ˆ í•„í„°ë§
        filtered_keywords = []
        for kw in related_keywords:
            if len(kw) >= 2 and not kw.isdigit():  # ìµœì†Œ 2ê¸€ì, ìˆ«ìë§Œ ìˆëŠ” í‚¤ì›Œë“œ ì œì™¸
                filtered_keywords.append(kw)
        
        # ìµœëŒ€ 5ê°œê¹Œì§€ ë°˜í™˜ (ì›ë³¸ í‚¤ì›Œë“œ ìš°ì„ )
        result = [keyword]  # ì›ë³¸ í‚¤ì›Œë“œë¥¼ ì²« ë²ˆì§¸ë¡œ
        for kw in filtered_keywords:
            if kw != keyword and len(result) < 5:
                result.append(kw)
        
        logger.info(f"í‚¤ì›Œë“œ '{keyword}' -> ê´€ë ¨ í‚¤ì›Œë“œ: {result}")
        return result
    
    def _search_naver_news_with_keywords(self, main_keyword: str, related_keywords: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)"""
        all_news = []
        seen_headlines = set()  # ì¤‘ë³µ í—¤ë“œë¼ì¸ ì¶”ì 
        
        # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ ì„¤ì •
        search_keywords = []
        
        # 1. ì›ë³¸ í‚¤ì›Œë“œ ìš°ì„ 
        if main_keyword:
            search_keywords.append(main_keyword)
        
        # 2. ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        for keyword in related_keywords:
            if keyword not in search_keywords:
                search_keywords.append(keyword)
        
        # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œë§Œ ê²€ìƒ‰
        search_keywords = search_keywords[:3]
        
        for keyword in search_keywords:
            if len(all_news) >= 3:  # ìµœëŒ€ 3ê°œ ë‰´ìŠ¤ë©´ ì¤‘ë‹¨
                break
                
            logger.info(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„")
            keyword_news = self._search_naver_news_simple(keyword)
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
            for news in keyword_news:
                if len(all_news) >= 3:
                    break
                # ì¤‘ë³µ ì²´í¬ (ì œëª© ê¸°ì¤€)
                headline = news.get('headline', '')
                if headline and headline not in seen_headlines:
                    seen_headlines.add(headline)
                    all_news.append(news)
        
        logger.info(f"í‚¤ì›Œë“œ '{main_keyword}'ë¡œ ìµœì¢… {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        return all_news
    
    def _search_naver_news_simple(self, keyword: str) -> List[Dict]:
        """ë‹¨ìˆœí•œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        if not keyword:
            return []
        
        for attempt in range(self.max_retries):
            try:
                encoded_keyword = urllib.parse.quote(keyword)
                url = f"https://search.naver.com/search.naver?where=news&query={encoded_keyword}"
                
                logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„ {attempt + 1}/{self.max_retries}: {keyword}")
                resp = self.session.get(url, timeout=self.timeout)
                resp.raise_for_status()
                
                soup = BeautifulSoup(resp.text, "html.parser")
                news_items = []
                
                # ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
                news_links = soup.select("a.news_tit")
                
                for link in news_links[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                    headline = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    # ìœ íš¨í•œ í—¤ë“œë¼ì¸ í•„í„°ë§
                    if (headline and 
                        len(headline) > 10 and 
                        'ì–¸ë¡ ì‚¬' not in headline and 
                        'êµ¬ë…' not in headline and
                        'ì„ ì •' not in headline):
                        
                        # URL ì •ê·œí™” ë° ë³´ì•ˆ ê²€ì‚¬
                        sanitized_url = self._sanitize_url(href)
                        if sanitized_url:
                            news_items.append({
                                'headline': headline,
                                'url': sanitized_url
                            })
                
                logger.info(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(news_items)}ê°œ")
                return news_items
                
            except requests.exceptions.Timeout:
                logger.warning(f"ë‰´ìŠ¤ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ({keyword}), ì¬ì‹œë„ {attempt + 1}/{self.max_retries}")
                if attempt == self.max_retries - 1:
                    break
            except requests.exceptions.RequestException as e:
                logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ({keyword}): {e}")
                break
            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
                break
        
        return []
    
    def analyze_news_sentiment(self, news_items: List[Dict], api_key: str = None) -> List[Dict]:
        """ë‰´ìŠ¤ ê°ì •ë¶„ì„ (GPT í™œìš©)"""
        if not news_items:
            return []
        
        if not api_key:
            import os
            api_key = os.getenv('OPENAI_API_KEY')
        
        if not api_key:
            logger.warning("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            client = openai.OpenAI(api_key=api_key)
            results = []
            
            for news_item in news_items[:3]:  # ìµœëŒ€ 3ê°œë§Œ ë¶„ì„
                headline = news_item.get('headline', '')
                url = news_item.get('url', '')
                
                if not headline:
                    continue
                
                try:
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": SYSTEM_PROMPT_ANALYSIS},
                            {"role": "user", "content": headline}
                        ],
                        max_tokens=100,
                        temperature=0.1
                    )
                    
                    result_text = response.choices[0].message.content.strip()
                    
                    # ê²°ê³¼ íŒŒì‹±
                    if "|" in result_text:
                        parts = result_text.split("|")
                        if len(parts) >= 3:
                            results.append({
                                'headline': parts[0].strip(),
                                'sentiment': parts[1].strip(),
                                'reason': parts[2].strip(),
                                'url': url,
                                'score': 0.8,
                                'confidence': 0.9
                            })
                
                except openai.RateLimitError:
                    logger.warning("OpenAI API rate limit ë„ë‹¬")
                    break
                except openai.APIError as e:
                    logger.error(f"OpenAI API ì˜¤ë¥˜: {e}")
                    continue
                except Exception as e:
                    logger.error(f"ê°œë³„ ë‰´ìŠ¤ ê°ì •ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê°ì •ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def generate_level_summary(self, news_items: List[Dict], level: int, api_key: str = None) -> str:
        """ë ˆë²¨ë³„ ë‰´ìŠ¤ ìš”ì•½ ìƒì„±"""
        if not news_items:
            return "ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        if not api_key:
            import os
            api_key = os.getenv('OPENAI_API_KEY')
        
        if not api_key:
            return "OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        try:
            # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ì¶”ì¶œ
            headlines = [item.get('headline', '') for item in news_items[:3] if item.get('headline')]
            if not headlines:
                return "ë¶„ì„í•  ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤."
            
            news_summary = " ".join(headlines)
            
            client = openai.OpenAI(api_key=api_key)
            level_prompt = LEVEL_PROMPTS.get(level, LEVEL_PROMPTS[3])
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": f"ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {level_prompt}"},
                    {"role": "user", "content": f"ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•´ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”: {news_summary}"}
                ],
                max_tokens=200,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except openai.RateLimitError:
            logger.warning("OpenAI API rate limit ë„ë‹¬")
            return "API ì‚¬ìš©ëŸ‰ í•œê³„ë¡œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except openai.APIError as e:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {e}")
            return "API ì˜¤ë¥˜ë¡œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"ë ˆë²¨ë³„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def display_news_analysis(self, code: str, level: int, mpti_type: str):
        """ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
        st.markdown(f'<div class="section-header">ğŸ“° ë‰´ìŠ¤ ê°ì •ë¶„ì„ <span class="level-indicator">Level {level}</span></div>', unsafe_allow_html=True)
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not code or not isinstance(code, str):
                st.error("ìœ íš¨í•˜ì§€ ì•Šì€ ì½”ë“œì…ë‹ˆë‹¤.")
                return
            
            # ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            with st.spinner("ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                news_items = self.fetch_naver_news(code)
            
            if news_items:
                # ê°ì •ë¶„ì„ ìˆ˜í–‰
                with st.spinner("ë‰´ìŠ¤ ê°ì •ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    sentiment_results = self.analyze_news_sentiment(news_items)
                
                if sentiment_results:
                    # ê°ì • ë¶„í¬ ê³„ì‚°
                    sentiment_counts = {}
                    for result in sentiment_results:
                        sentiment = result.get('sentiment', '')
                        if sentiment:
                            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
                    
                    # ê°ì • ë¶„í¬ ì„¹ì…˜
                    st.markdown("### ğŸ“Š ê°ì • ë¶„í¬")
                    
                    # ê°ì •ë³„ ì¹´ë“œ
                    if sentiment_counts:
                        cols = st.columns(len(sentiment_counts))
                        sentiment_config = {
                            'ê¸ì •': {'color': '#28a745', 'icon': 'ğŸ˜Š', 'bg_color': '#d4edda'},
                            'ë¶€ì •': {'color': '#dc3545', 'icon': 'ğŸ˜', 'bg_color': '#f8d7da'},
                            'ì¤‘ë¦½': {'color': '#6c757d', 'icon': 'ğŸ˜', 'bg_color': '#e2e3e5'}
                        }
                        
                        for i, (sentiment, count) in enumerate(sentiment_counts.items()):
                            with cols[i]:
                                config = sentiment_config.get(sentiment, {'color': '#6c757d', 'icon': 'âšª', 'bg_color': '#f8f9fa'})
                                st.markdown(f"""
                                <div style="
                                    background: {config['bg_color']};
                                    border: 2px solid {config['color']};
                                    border-radius: 10px;
                                    padding: 1rem;
                                    text-align: center;
                                    margin: 0.5rem 0;">
                                    <h3 style="color: {config['color']}; margin: 0;">{config['icon']} {sentiment}</h3>
                                    <h2 style="color: {config['color']}; margin: 0.5rem 0;">{count}</h2>
                                    <p style="margin: 0; color: #666;">ê°œ ë‰´ìŠ¤</p>
                                </div>
                                """, unsafe_allow_html=True)
                        
                        # íŒŒì´ ì°¨íŠ¸
                        if len(sentiment_counts) > 1:
                            try:
                                import plotly.graph_objects as go
                                
                                colors = [sentiment_config.get(s, {}).get('color', '#6c757d') for s in sentiment_counts.keys()]
                                
                                fig = go.Figure(data=[go.Pie(
                                    labels=list(sentiment_counts.keys()),
                                    values=list(sentiment_counts.values()),
                                    hole=0.4,
                                    marker_colors=colors,
                                    textinfo='label+percent',
                                    textfont_size=14
                                )])
                                
                                fig.update_layout(
                                    title="ë‰´ìŠ¤ ê°ì • ë¶„í¬",
                                    showlegend=True,
                                    height=400,
                                    margin=dict(t=50, b=50, l=50, r=50)
                                )
                                
                                st.plotly_chart(fig, use_container_width=True)
                            except Exception as e:
                                logger.error(f"ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                                st.warning("ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ìƒì„¸ ë¶„ì„ ê²°ê³¼
                    st.markdown("### ğŸ“ ìƒì„¸ ë¶„ì„ ê²°ê³¼")
                    
                    for i, result in enumerate(sentiment_results[:3], 1):
                        sentiment = result.get('sentiment', '')
                        config = sentiment_config.get(sentiment, {'color': '#6c757d', 'icon': 'âšª', 'bg_color': '#f8f9fa'})
                        url = result.get('url', '')
                        
                        # ë‰´ìŠ¤ ì¹´ë“œ
                        st.markdown(f"""
                        <div style="
                            background: {config['bg_color']};
                            border-left: 5px solid {config['color']};
                            border-radius: 8px;
                            padding: 1.5rem;
                            margin: 1rem 0;
                            box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                            <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
                                <span style="font-size: 1.5rem; margin-right: 0.5rem;">{config['icon']}</span>
                                <h4 style="margin: 0; color: {config['color']};">{sentiment}</h4>
                            </div>
                            <h5 style="margin: 0.5rem 0; color: #333;">{result.get('headline', '')}</h5>
                            <p style="margin: 0.5rem 0; color: #666; font-size: 0.9rem;">ğŸ’¡ {result.get('reason', '')}</p>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # ë§í¬ ë²„íŠ¼
                        if url and self._is_valid_url(url):
                            if st.button(f"ğŸ”— ë‰´ìŠ¤ ë³´ê¸° ({i})", key=f"news_link_{i}"):
                                st.markdown(f"[ë‰´ìŠ¤ ì›ë¬¸ ë³´ê¸°]({url})")
                    
                    # ë ˆë²¨ë³„ ìš”ì•½
                    summary = self.generate_level_summary(news_items, level)
                    if summary:
                        st.markdown("### ğŸ“‹ ì¢…í•© ìš”ì•½")
                        st.markdown(f"""
                        <div style="
                            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                            color: white;
                            padding: 1.5rem;
                            border-radius: 10px;
                            margin: 1rem 0;">
                            <h4 style="margin: 0 0 1rem 0;">ğŸ¯ AI ë¶„ì„ ìš”ì•½</h4>
                            <p style="margin: 0; line-height: 1.6;">{summary}</p>
                        </div>
                        """, unsafe_allow_html=True)
                
                else:
                    st.warning("ë‰´ìŠ¤ ê°ì •ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ìµœê·¼ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            st.error(f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜: {e}")

