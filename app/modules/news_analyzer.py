"""
ë‰´ìŠ¤ ë¶„ì„ ëª¨ë“ˆ
- ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° GPT ê°ì •ë¶„ì„
- ë ˆë²¨ë³„ ë§ì¶¤í˜• ìš”ì•½
"""

import streamlit as st
import requests
import logging
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
import openai
import urllib.parse
import re
from urllib.parse import urlparse
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# ë ˆë²¨ë³„ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸
try:
    from chatbot.config import Config
    LEVEL_PROMPTS = Config.LEVEL_PROMPTS
except ImportError:
    LEVEL_PROMPTS = {
        1: """- Level 1 (ì´ˆë³´ì): 
        â€¢ ì–´íˆ¬: ìœ ì¹˜ì›/ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ì•„ì£¼ ì‰¬ìš´ ë§ë¡œ ì„¤ëª…
        â€¢ ë‚´ìš©: íˆ¬ì ê¸°ì´ˆ ê°œë… ìœ„ì£¼, ë³µì¡í•œ ìš©ì–´ëŠ” ë¹„ìœ ì™€ ì˜ˆì‹œë¡œ ëŒ€ì²´
        â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ í•µì‹¬ë§Œ ìš”ì•½""",
        
        2: """- Level 2 (ì…ë¬¸ì): 
        â€¢ ì–´íˆ¬: ì¤‘ê³ ë“±í•™ìƒë„ ì´í•´ ê°€ëŠ¥í•œ ì‰¬ìš´ ë§ë¡œ ì„¤ëª…
        â€¢ ë‚´ìš©: í•µì‹¬ ê°œë…ê³¼ ì´ìœ ë¥¼ í¬í•¨, ê¸°ë³¸ì ì¸ íˆ¬ì ì§€ì‹ ì „ë‹¬
        â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ì„¤ëª…""",
        
        3: """- Level 3 (ì¤‘ê¸‰ì): 
        â€¢ ì–´íˆ¬: ì¼ë°˜ ì„±ì¸ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…
        â€¢ ë‚´ìš©: ì‹¤ì „ íŒê³¼ êµ¬ì²´ì  ì „ëµ í¬í•¨, ë°ì´í„° ê¸°ë°˜ ë¶„ì„
        â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ë¶„ì„""",
        
        4: """- Level 4 (ê³ ê¸‰ì): 
        â€¢ ì–´íˆ¬: íˆ¬ì ê²½í—˜ì´ ìˆëŠ” ì„±ì¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì „ë¬¸ì  ì„¤ëª…
        â€¢ ë‚´ìš©: ì‹¬í™” ë¶„ì„ê³¼ ê³ ê¸‰ ì „ëµ, ì‹œì¥ ë™í–¥ê³¼ ì—°ê´€ì„± ë¶„ì„
        â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ìƒì„¸ ì„¤ëª…""",
        
        5: """- Level 5 (ì „ë¬¸ê°€): 
        â€¢ ì–´íˆ¬: íˆ¬ì ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ê³ ê¸‰ ë¶„ì„ê³¼ ì „ë¬¸ ìš©ì–´ ì‚¬ìš©
        â€¢ ë‚´ìš©: ìµœê³  ìˆ˜ì¤€ ë¶„ì„ê³¼ ì‹¤ì „ í™œìš©, ì‹œì¥ ë¯¸ì‹œêµ¬ì¡°ê¹Œì§€ ê³ ë ¤
        â€¢ ê¸¸ì´: 1-2ì¤„ ì´ìƒìœ¼ë¡œ ì „ë¬¸ì  ì„¤ëª…"""
    }

SYSTEM_PROMPT_ANALYSIS = """
- ë‹¹ì‹ ì€ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ê°ì„± ë¶„ì„ê¸°ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ ë³´ë‚¸ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë³´ê³ , ë‹¤ìŒ 3ê°œì˜ í•„ë“œë§Œ "|" ë¡œ êµ¬ë¶„í•´ í•œ ì¤„ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
  1) ë‰´ìŠ¤ê¸°ì‚¬(ì›ë¬¸ ê·¸ëŒ€ë¡œ)
  2) ê¸ë¶€ì • ê²°ê³¼ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜)
  3) ì´ìœ  (í•œ ë¬¸ì¥)
- ë°˜ë“œì‹œ ìœ„ ìˆœì„œëŒ€ë¡œ "ë‰´ìŠ¤ê¸°ì‚¬|ê¸ë¶€ì • ê²°ê³¼|ì´ìœ " í˜•íƒœë¡œë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.
"""

class NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.max_retries = 3
        self.timeout = 10
        self.request_delay = 2
        

        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        
        import random
        selected_agent = random.choice(user_agents)
        
        # ë” í˜„ì‹¤ì ì¸ í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': selected_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Referer': 'https://www.naver.com/',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        })
        
        # ì„¸ì…˜ ì¿ í‚¤ ì„¤ì • (ë„¤ì´ë²„ ì°¨ë‹¨ ìš°íšŒ)
        self.session.cookies.update({
            'NNB': 'random_string_here',
            'nx_ssl': '2',
            'ASID': 'random_string_here'
        })
        
        self.timeout = 20  # 20ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¦ê°€
        self.max_retries = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        self.request_delay = 3  # ìš”ì²­ ê°„ê²© ì¦ê°€ (3ì´ˆ)
    
    def _is_valid_url(self, url: str) -> bool:
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            parsed = urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except Exception:
            return False
    
    def _sanitize_url(self, url: str) -> str:
        """URL ì •ê·œí™” ë° ë³´ì•ˆ ê²€ì‚¬"""
        if not url:
            return ""
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if url.startswith('/'):
            url = f"https://search.naver.com{url}"
        elif not url.startswith('http'):
            url = f"https://search.naver.com/{url}"
        
        # URL ìœ íš¨ì„± ê²€ì‚¬
        if not self._is_valid_url(url):
            return ""
        
        # í—ˆìš©ëœ ë„ë©”ì¸ë§Œ í—ˆìš©
        allowed_domains = ['search.naver.com', 'news.naver.com', 'finance.naver.com']
        parsed = urlparse(url)
        if parsed.netloc not in allowed_domains:
            return ""
        
        return url
    
    def _is_relevant_news(self, headline: str, keyword: str) -> bool:
        """ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì´ í‚¤ì›Œë“œì™€ ê´€ë ¨ìˆëŠ”ì§€ ê²€ì¦"""
        if not headline or not keyword:
            return False
        
        headline_lower = headline.lower()
        keyword_lower = keyword.lower()
        
        # í‚¤ì›Œë“œë³„ ê´€ë ¨ ë‹¨ì–´ ì •ì˜
        keyword_related_words = {
            'kbstar': ['kbstar', 'kbìŠ¤íƒ€', 'kb star', 'etf', 'íˆ¬ì', 'ì£¼ì‹', 'í€ë“œ'],
            '200': ['200', 'kospi', 'ì½”ìŠ¤í”¼', 'ì§€ìˆ˜', 'ì‹œì¥'],
            'ë°˜ë„ì²´': ['ë°˜ë„ì²´', 'ì‚¼ì„±ì „ì', 'skí•˜ì´ë‹‰ìŠ¤', 'ë©”ëª¨ë¦¬', 'ì¹©'],
            '2ì°¨ì „ì§€': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'lgì—ë„ˆì§€ì†”ë£¨ì…˜', 'ì‚¼ì„±sdi', 'ì „ê¸°ì°¨'],
            'ì‚¼ì„±ì „ì': ['ì‚¼ì„±ì „ì', 'ì‚¼ì„±', 'ì „ì', 'ë°˜ë„ì²´', 'ë©”ëª¨ë¦¬'],
            'skí•˜ì´ë‹‰ìŠ¤': ['skí•˜ì´ë‹‰ìŠ¤', 'sk', 'í•˜ì´ë‹‰ìŠ¤', 'ë©”ëª¨ë¦¬', 'ë°˜ë„ì²´'],
            'lgì—ë„ˆì§€ì†”ë£¨ì…˜': ['lgì—ë„ˆì§€ì†”ë£¨ì…˜', 'lg', 'ì—ë„ˆì§€', 'ë°°í„°ë¦¬', '2ì°¨ì „ì§€']
        }
        
        # í‚¤ì›Œë“œì—ì„œ ê´€ë ¨ ë‹¨ì–´ ì°¾ê¸°
        related_words = []
        for key, words in keyword_related_words.items():
            if key in keyword_lower:
                related_words.extend(words)
        
        # ê¸°ë³¸ ê´€ë ¨ ë‹¨ì–´ ì¶”ê°€
        related_words.extend(['íˆ¬ì', 'ì£¼ì‹', 'etf', 'í€ë“œ', 'ê¸ˆìœµ', 'ì‹œì¥'])
        
        # í—¤ë“œë¼ì¸ì— ê´€ë ¨ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        for word in related_words:
            if word in headline_lower:
                return True
        
        # í‚¤ì›Œë“œ ìì²´ê°€ í—¤ë“œë¼ì¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if keyword_lower in headline_lower:
            return True
        
        return False
    
    def _search_naver_finance_news(self, keyword: str) -> List[Dict]:
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ (Jupyter Notebook ì½”ë“œ ì°¸ê³ )"""
        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ URL (ì‚¬ìš©ì ì½”ë“œì™€ ë™ì¼)
            url = f"https://finance.naver.com/item/news_news.naver?code={keyword}"
            headers = {
                "User-Agent": "Mozilla/5.0",
                "Referer": url
            }
            
            resp = self.session.get(url, headers=headers, timeout=self.timeout)
            resp.raise_for_status()
            
            soup = BeautifulSoup(resp.text, "html.parser")
            news_items = []
            
            # ì‚¬ìš©ì ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
            for row in soup.select("table.type5 tbody tr"):
                title_tag = row.select_one("td.title a.tit")
                date_tag = row.select_one("td.date")
                
                if not title_tag or not date_tag:
                    continue
                
                try:
                    # ë‚ ì§œ íŒŒì‹± (ì‚¬ìš©ì ì½”ë“œì™€ ë™ì¼)
                    date_str = date_tag.text.strip()
                    dt = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
                    
                    # 14ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ
                    if dt < datetime.now() - timedelta(days=14):
                        continue
                    
                    headline = title_tag.text.strip()
                    
                    # URL ìƒì„± (ì‚¬ìš©ì ì½”ë“œì™€ ë™ì¼)
                    href = title_tag.get('href', '')
                    if href.startswith('/'):
                        href = f"https://finance.naver.com{href}"
                    
                    news_items.append({
                        'headline': headline,
                        'url': href,
                        'date': dt
                    })
                
                except ValueError:
                    continue
            
            # ë‚ ì§œìˆœ ì •ë ¬ í›„ ìµœê·¼ 3ê°œ ë°˜í™˜
            news_items.sort(key=lambda x: x['date'], reverse=True)
            result = []
            for item in news_items[:3]:
                result.append({
                    'headline': item['headline'],
                    'url': item['url']
                })
            
            if result:
                logger.info(f"ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ: {len(result)}ê°œ")
                return result
            
        except Exception as e:
            logger.debug(f"ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
        
        return []
    
    def fetch_naver_news(self, code: str) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ê³¼ ë§í¬ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not code or not isinstance(code, str):
                logger.warning("ìœ íš¨í•˜ì§€ ì•Šì€ ì½”ë“œ ì…ë ¥")
                return []
            
            # ì¢…ëª©ëª…ì„ ì¢…ëª©ì½”ë“œë¡œ ë³€í™˜
            stock_code_mapping = {
                'ì‚¼ì„±ì „ì': '005930',
                'SKí•˜ì´ë‹‰ìŠ¤': '000660',
                'í•œë¯¸ë°˜ë„ì²´': '042700',
                'NAVER': '035420',
                'ì¹´ì¹´ì˜¤': '035720',
                'LGì—ë„ˆì§€ì†”ë£¨ì…˜': '373220',
                'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤': '207940',
                'í˜„ëŒ€ì°¨': '005380',
                'ê¸°ì•„': '000270',
                'POSCOí™€ë”©ìŠ¤': '005490',
                'LGí™”í•™': '051910',
                'ì‚¼ì„±SDI': '006400',
                'SKì´ë…¸ë² ì´ì…˜': '096770',
                'LGì „ì': '066570',
                'ì‚¼ì„±ìƒëª…': '032830',
                'KBê¸ˆìœµ': '105560',
                'ì‹ í•œì§€ì£¼': '055550',
                'í•˜ë‚˜ê¸ˆìœµì§€ì£¼': '086790'
            }
            
            # ì¢…ëª©ëª…ì´ë©´ ì¢…ëª©ì½”ë“œë¡œ ë³€í™˜
            if code in stock_code_mapping:
                code = stock_code_mapping[code]
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (ETF, ë°˜ë„ì²´ ë“±)
            if not code.isdigit() and ('ETF' in code.upper() or 'ë°˜ë„ì²´' in code or '2ì°¨ì „ì§€' in code or 'KOSPI' in code.upper()):
                # í‚¤ì›Œë“œë³„ ê´€ë ¨ ê²€ìƒ‰ì–´ ì¶”ê°€
                related_keywords = self._get_related_keywords(code)
                news_items = self._search_naver_news_with_keywords(code, related_keywords)
                
                # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                if not news_items:
                    logger.warning(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {code}")
                    return []
                
                return news_items
            
            # ì¢…ëª© ì½”ë“œ ê¸°ë°˜ ê²€ìƒ‰
            if code.isdigit():
                # ì¢…ëª©ì½”ë“œë¡œ ì§ì ‘ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰
                news_items = self._search_naver_finance_news(code)
                
                # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                if not news_items:
                    logger.warning(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {code}")
                    return []
                
                return news_items
            
            # ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            news_items = self._search_naver_news_simple('ì£¼ì‹ íˆ¬ì')
            
            # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            if not news_items:
                logger.warning(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {code}")
                return []
            
            return news_items
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨ ({code}): {e}")
            return []
    
    def _get_stock_name(self, code: str) -> str:
        """ì¢…ëª© ì½”ë“œë¡œ ì¢…ëª©ëª… ê°€ì ¸ì˜¤ê¸°"""
        stock_names = {
            '091160': 'KBSTAR 200',
            '091170': 'KBSTAR ì½”ìŠ¤ë‹¥150',
            '091230': 'KBSTAR ë°˜ë„ì²´',
            '306540': 'KBSTAR 2ì°¨ì „ì§€í…Œë§ˆ',
            '233740': 'KBSTAR K-ë‰´ë”œë””ì§€í„¸í”ŒëŸ¬ìŠ¤',
            '005930': 'ì‚¼ì„±ì „ì',
            '000660': 'SKí•˜ì´ë‹‰ìŠ¤',
            '035420': 'NAVER',
            '035720': 'ì¹´ì¹´ì˜¤',
            '373220': 'LGì—ë„ˆì§€ì†”ë£¨ì…˜'
        }
        return stock_names.get(code, '')
    
    def _get_keyword_mapping(self) -> Dict[str, List[str]]:
        """í‚¤ì›Œë“œ ë§¤í•‘ ê·œì¹™ ì •ì˜"""
        return {
            # ETF ê´€ë ¨ í‚¤ì›Œë“œ
            'etf': {
                'ë°˜ë„ì²´': ['ë°˜ë„ì²´', 'ë°˜ë„ì²´ì£¼', 'ë°˜ë„ì²´ ETF'],
                '2ì°¨ì „ì§€': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', '2ì°¨ì „ì§€ ETF'],
                'ë°°í„°ë¦¬': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'ë°°í„°ë¦¬ ETF'],
                'kospi': ['KOSPI', 'ì½”ìŠ¤í”¼', 'ëŒ€í˜•ì£¼', 'KOSPI ETF'],
                'kosdaq': ['KOSDAQ', 'ì½”ìŠ¤ë‹¥', 'ì¤‘ì†Œí˜•ì£¼', 'KOSDAQ ETF'],
                'default': ['ì£¼ì‹', 'íˆ¬ì']
            },
            # ì„¹í„°ë³„ í‚¤ì›Œë“œ
            'ë°˜ë„ì²´': ['ë°˜ë„ì²´', 'ë°˜ë„ì²´ì£¼', 'ë°˜ë„ì²´ ì‚°ì—…', 'ë©”ëª¨ë¦¬'],
            '2ì°¨ì „ì§€': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'ì „ê¸°ì°¨ ë°°í„°ë¦¬', 'ë¦¬íŠ¬'],
            'ë°°í„°ë¦¬': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'ì „ê¸°ì°¨ ë°°í„°ë¦¬', 'ë¦¬íŠ¬'],
            'kospi': ['KOSPI', 'ì½”ìŠ¤í”¼', 'ëŒ€í˜•ì£¼', 'ì£¼ì‹ì‹œì¥'],
            'kosdaq': ['KOSDAQ', 'ì½”ìŠ¤ë‹¥', 'ì¤‘ì†Œí˜•ì£¼', 'ê¸°ìˆ ì£¼'],
            'ai': ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹'],
            'ë°”ì´ì˜¤': ['ë°”ì´ì˜¤', 'ì œì•½', 'ì˜ë£Œ', 'í—¬ìŠ¤ì¼€ì–´'],
            'ê²Œì„': ['ê²Œì„', 'ê²Œì„ì£¼', 'ëª¨ë°”ì¼ê²Œì„', 'ì½˜í…ì¸ '],
            'ì „ê¸°ì°¨': ['ì „ê¸°ì°¨', 'EV', 'í…ŒìŠ¬ë¼', 'ì „ê¸°ìë™ì°¨'],
            'ì‹ ì¬ìƒ': ['ì‹ ì¬ìƒ', 'íƒœì–‘ê´‘', 'í’ë ¥', 'ì¹œí™˜ê²½'],
            'ê¸ˆìœµ': ['ê¸ˆìœµ', 'ì€í–‰', 'ë³´í—˜', 'ì¦ê¶Œ'],
            'ë¶€ë™ì‚°': ['ë¶€ë™ì‚°', 'REITs', 'ì•„íŒŒíŠ¸', 'ê±´ì„¤']
        }
    
    def _extract_primary_keyword(self, keyword: str) -> str:
        """í‚¤ì›Œë“œì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keyword_lower = keyword.lower()
        
        # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ í‚¤ì›Œë“œë¶€í„° ë§¤ì¹­
        priority_keywords = [
            'ë°˜ë„ì²´', '2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'kospi', 'kosdaq', 
            'ai', 'ë°”ì´ì˜¤', 'ê²Œì„', 'ì „ê¸°ì°¨', 'ì‹ ì¬ìƒ', 'ê¸ˆìœµ', 'ë¶€ë™ì‚°'
        ]
        
        for primary in priority_keywords:
            if primary in keyword_lower:
                return primary
        
        # ETF í‚¤ì›Œë“œ ì²´í¬
        if 'etf' in keyword_lower:
            return 'etf'
        
        return 'default'
    
    def _get_related_keywords(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œì— ë”°ë¥¸ ê´€ë ¨ ê²€ìƒ‰ì–´ ìƒì„± """
        if not keyword or not isinstance(keyword, str):
            return []
        
        # í‚¤ì›Œë“œ ì •ê·œí™”
        keyword = keyword.strip()
        if not keyword:
            return []
        
        related_keywords = set([keyword])  # ì›ë³¸ í‚¤ì›Œë“œ 
        keyword_mapping = self._get_keyword_mapping()
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        primary_keyword = self._extract_primary_keyword(keyword)
        
        # ETF í‚¤ì›Œë“œì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if primary_keyword == 'etf':
            etf_mapping = keyword_mapping.get('etf', {})
            
            # ETF í‚¤ì›Œë“œì—ì„œ ì„¸ë¶€ ì£¼ì œ ì¶”ì¶œ
            keyword_lower = keyword.lower()
            for etf_type, related_list in etf_mapping.items():
                if etf_type != 'default' and etf_type in keyword_lower:
                    related_keywords.update(related_list)
                    break
            else:
                # ê¸°ë³¸ ETF í‚¤ì›Œë“œ
                related_keywords.update(etf_mapping.get('default', ['ì£¼ì‹', 'íˆ¬ì']))
        
        # ì¼ë°˜ ì„¹í„° í‚¤ì›Œë“œ ì²˜ë¦¬
        elif primary_keyword in keyword_mapping:
            related_keywords.update(keyword_mapping[primary_keyword])
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ê°€ 
        if primary_keyword != 'default':
            related_keywords.add('íˆ¬ì')
        
        # í‚¤ì›Œë“œ í’ˆì§ˆ í•„í„°ë§
        filtered_keywords = []
        for kw in related_keywords:
            if len(kw) >= 2 and not kw.isdigit():  # ìµœì†Œ 2ê¸€ì, ìˆ«ìë§Œ ìˆëŠ” í‚¤ì›Œë“œ ì œì™¸
                filtered_keywords.append(kw)
        
        # ìµœëŒ€ 5ê°œê¹Œì§€ ë°˜í™˜
        result = [keyword]  # ì›ë³¸ í‚¤ì›Œë“œë¥¼ ì²« ë²ˆì§¸ë¡œ
        for kw in filtered_keywords:
            if kw != keyword and len(result) < 5:
                result.append(kw)
        
        logger.info(f"í‚¤ì›Œë“œ '{keyword}' -> ê´€ë ¨ í‚¤ì›Œë“œ: {result}")
        return result
    
    def _search_naver_news_with_keywords(self, main_keyword: str, related_keywords: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)"""
        all_news = []
        seen_headlines = set()  # ì¤‘ë³µ í—¤ë“œë¼ì¸ ì¶”ì 
        
        # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ ì„¤ì •
        search_keywords = []
        
        # 1. ì›ë³¸ í‚¤ì›Œë“œ ìš°ì„ 
        if main_keyword:
            search_keywords.append(main_keyword)
        
        # 2. ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
        for keyword in related_keywords:
            if keyword not in search_keywords:
                search_keywords.append(keyword)
        
        # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œë§Œ ê²€ìƒ‰
        search_keywords = search_keywords[:3]
        
        for keyword in search_keywords:
            if len(all_news) >= 3:  # ìµœëŒ€ 3ê°œ ë‰´ìŠ¤ë©´ ì¤‘ë‹¨
                break
                
            logger.info(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„")
            keyword_news = self._search_naver_news_simple(keyword)
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
            for news in keyword_news:
                if len(all_news) >= 3:
                    break
                # ì¤‘ë³µ ì²´í¬ (ì œëª© ê¸°ì¤€)
                headline = news.get('headline', '')
                if headline and headline not in seen_headlines:
                    seen_headlines.add(headline)
                    all_news.append(news)
        
        logger.info(f"í‚¤ì›Œë“œ '{main_keyword}'ë¡œ ìµœì¢… {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        return all_news
    
    def _search_naver_news_simple(self, keyword: str) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        if not keyword:
            return []
        
        # ë¨¼ì € ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì‹œë„ (ì‚¬ìš©ì ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹)
        finance_news = self._search_naver_finance_news(keyword)
        if finance_news:
            return finance_news
        
        # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ê²€ìƒ‰ ì‹œë„
        for attempt in range(self.max_retries):
            try:
                # ìš”ì²­ ê°„ê²© ì¶”ê°€ (ì²« ë²ˆì§¸ ìš”ì²­ ì œì™¸)
                if attempt > 0:
                    import time
                    time.sleep(self.request_delay)
                
                encoded_keyword = urllib.parse.quote(keyword)
                url = f"https://search.naver.com/search.naver?where=news&query={encoded_keyword}"
                
                logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„ {attempt + 1}/{self.max_retries}: {keyword}")
                
                # ìš”ì²­ ì „ì— ì„¸ì…˜ í—¤ë” ì¬ì„¤ì • (403 ì˜¤ë¥˜ ë°©ì§€)
                if attempt > 0:
                    import random
                    user_agents = [
                        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                    ]
                    self.session.headers['User-Agent'] = random.choice(user_agents)
                
                resp = self.session.get(url, timeout=self.timeout)
                
                # 403 ì˜¤ë¥˜ íŠ¹ë³„ ì²˜ë¦¬
                if resp.status_code == 403:
                    logger.warning(f"403 Forbidden ì˜¤ë¥˜ ë°œìƒ ({keyword}), ëŒ€ì²´ ë°©ë²• ì‹œë„")
                    # ëŒ€ì²´ ê²€ìƒ‰ ë°©ë²• ì‹œë„
                    alternative_news = self._search_alternative_news(keyword)
                    if alternative_news:
                        return alternative_news
                    continue
                
                resp.raise_for_status()
                
                soup = BeautifulSoup(resp.text, "html.parser")
                news_items = []
                
                # ë‰´ìŠ¤ ë§í¬ ì°¾ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                news_links = soup.select("a.news_tit")
                if not news_links:
                    news_links = soup.select(".news_area a")
                if not news_links:
                    news_links = soup.select("a[href*='news.naver.com']")
                
                for link in news_links[:5]:  # ë” ë§ì€ ë‰´ìŠ¤ ìˆ˜ì§‘ í›„ í•„í„°ë§
                    headline = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    # ê°•í™”ëœ í—¤ë“œë¼ì¸ í•„í„°ë§
                    if (headline and 
                        len(headline) > 10 and 
                        'ì–¸ë¡ ì‚¬' not in headline and 
                        'êµ¬ë…' not in headline and
                        'ì„ ì •' not in headline and
                        'ë‚ ì”¨' not in headline and  # ë‚ ì”¨ ë‰´ìŠ¤ ì œì™¸
                        'ë¶í•œ' not in headline and  # ë¶í•œ ê´€ë ¨ ë‰´ìŠ¤ ì œì™¸
                        'ì •ì¹˜' not in headline and  # ì •ì¹˜ ë‰´ìŠ¤ ì œì™¸
                        'ì‚¬íšŒ' not in headline):    # ì‚¬íšŒ ë‰´ìŠ¤ ì œì™¸
                        
                        # í‚¤ì›Œë“œ ê´€ë ¨ì„± ê²€ì¦
                        if self._is_relevant_news(headline, keyword):
                            # URL ì •ê·œí™” ë° ë³´ì•ˆ ê²€ì‚¬
                            sanitized_url = self._sanitize_url(href)
                            if sanitized_url:
                                news_items.append({
                                    'headline': headline,
                                    'url': sanitized_url
                                })
                
                # ìµœëŒ€ 3ê°œë§Œ ë°˜í™˜
                if news_items:
                    logger.info(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(news_items)}ê°œ")
                    return news_items[:3]
                
            except requests.exceptions.Timeout:
                logger.warning(f"ë‰´ìŠ¤ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ({keyword}), ì¬ì‹œë„ {attempt + 1}/{self.max_retries}")
                if attempt == self.max_retries - 1:
                    break
            except requests.exceptions.RequestException as e:
                logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ({keyword}): {e}")
                if "403" in str(e):
                    logger.warning("403 ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ì²´ ë°©ë²• ì‹œë„")
                    alternative_news = self._search_alternative_news(keyword)
                    if alternative_news:
                        return alternative_news
                break
            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
                break
        
        return []
    
    def _search_alternative_news(self, keyword: str) -> List[Dict]:
        """ëŒ€ì²´ ë‰´ìŠ¤ ê²€ìƒ‰ ë°©ë²• (403 ì˜¤ë¥˜ ì‹œ ì‚¬ìš©)"""
        try:
            # ë” ê°„ë‹¨í•œ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„
            simple_keywords = self._get_simple_keywords(keyword)
            
            for simple_keyword in simple_keywords:
                try:
                    # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ ì§ì ‘ URL ì‹œë„
                    encoded_keyword = urllib.parse.quote(simple_keyword)
                    url = f"https://news.naver.com/main/search/search.naver?query={encoded_keyword}"
                    
                    resp = self.session.get(url, timeout=self.timeout)
                    if resp.status_code == 200:
                        soup = BeautifulSoup(resp.text, "html.parser")
                        news_items = []
                        
                        # ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
                        news_links = soup.select(".news_area a, .news_tit")
                        
                        for link in news_links[:3]:
                            headline = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if (headline and len(headline) > 10):
                                sanitized_url = self._sanitize_url(href)
                                if sanitized_url:
                                    news_items.append({
                                        'headline': headline,
                                        'url': sanitized_url
                                    })
                        
                        if news_items:
                            logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ë¡œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                            return news_items
                
                except Exception as e:
                    logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ '{simple_keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            # 2. í•œêµ­ê²½ì œ ë‰´ìŠ¤ ì‹œë„
            for simple_keyword in simple_keywords:
                try:
                    encoded_keyword = urllib.parse.quote(simple_keyword)
                    url = f"https://search.hankyung.com/apps.frm/search.news?query={encoded_keyword}"
                    
                    resp = self.session.get(url, timeout=self.timeout)
                    if resp.status_code == 200:
                        soup = BeautifulSoup(resp.text, "html.parser")
                        news_items = []
                        
                        # í•œêµ­ê²½ì œ ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
                        news_links = soup.select(".news_tit a, .tit a")
                        
                        for link in news_links[:3]:
                            headline = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if (headline and len(headline) > 10):
                                # í•œêµ­ê²½ì œ URL ì •ê·œí™”
                                if href.startswith('/'):
                                    href = f"https://www.hankyung.com{href}"
                                elif not href.startswith('http'):
                                    href = f"https://www.hankyung.com/{href}"
                                
                                news_items.append({
                                    'headline': headline,
                                    'url': href
                                })
                        
                        if news_items:
                            logger.info(f"í•œêµ­ê²½ì œë¡œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                            return news_items
                
                except Exception as e:
                    logger.debug(f"í•œêµ­ê²½ì œ '{simple_keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            # 3. ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ ì‹œë„
            for simple_keyword in simple_keywords:
                try:
                    encoded_keyword = urllib.parse.quote(simple_keyword)
                    url = f"https://www.mk.co.kr/search/?word={encoded_keyword}"
                    
                    resp = self.session.get(url, timeout=self.timeout)
                    if resp.status_code == 200:
                        soup = BeautifulSoup(resp.text, "html.parser")
                        news_items = []
                        
                        # ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
                        news_links = soup.select(".news_ttl a, .tit a")
                        
                        for link in news_links[:3]:
                            headline = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if (headline and len(headline) > 10):
                                # ë§¤ì¼ê²½ì œ URL ì •ê·œí™”
                                if href.startswith('/'):
                                    href = f"https://www.mk.co.kr{href}"
                                elif not href.startswith('http'):
                                    href = f"https://www.mk.co.kr/{href}"
                                
                                news_items.append({
                                    'headline': headline,
                                    'url': href
                                })
                        
                        if news_items:
                            logger.info(f"ë§¤ì¼ê²½ì œë¡œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                            return news_items
                
                except Exception as e:
                    logger.debug(f"ë§¤ì¼ê²½ì œ '{simple_keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            # 4. ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹¤íŒ¨
            logger.warning("ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹¤íŒ¨")
            return []
            
        except Exception as e:
            logger.error(f"ëŒ€ì²´ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    

    
    def _get_simple_keywords(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œë¥¼ ë” ê°„ë‹¨í•œ í˜•íƒœë¡œ ë³€í™˜"""
        # ë³µì¡í•œ í‚¤ì›Œë“œë¥¼ ë‹¨ìˆœí™”
        if 'KBSTAR' in keyword:
            if 'ë°˜ë„ì²´' in keyword:
                return ['ë°˜ë„ì²´', 'ë°˜ë„ì²´ ETF', 'ë°˜ë„ì²´ì£¼']
            elif '200' in keyword:
                return ['KOSPI', 'ëŒ€í˜•ì£¼', 'ì£¼ì‹ì‹œì¥']
            else:
                return ['ETF', 'ì£¼ì‹', 'íˆ¬ì']
        
        # ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ë‹¨ìˆœí™”
        simple_mapping = {
            'ë°˜ë„ì²´ ETF': ['ë°˜ë„ì²´', 'ë°˜ë„ì²´ì£¼'],
            '2ì°¨ì „ì§€ ETF': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬'],
            'KOSPI ETF': ['KOSPI', 'ì£¼ì‹ì‹œì¥'],
            'KOSDAQ ETF': ['KOSDAQ', 'ê¸°ìˆ ì£¼']
        }
        
        for complex_key, simple_keys in simple_mapping.items():
            if complex_key in keyword:
                return simple_keys
        
        return [keyword.split()[0] if keyword.split() else keyword]
    
    def analyze_news_sentiment(self, news_items: List[Dict], api_key: str = None) -> List[Dict]:
        """ë‰´ìŠ¤ ê°ì •ë¶„ì„ (GPT í™œìš©)"""
        if not news_items:
            return []
        
        if not api_key:
            import os
            api_key = os.getenv('OPENAI_API_KEY')
        
        if not api_key:
            logger.warning("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            client = openai.OpenAI(api_key=api_key)
            results = []
            
            logger.info(f"ì´ {len(news_items)}ê°œ ë‰´ìŠ¤ì— ëŒ€í•´ ê°ì •ë¶„ì„ ì‹œì‘")
            
            for i, news_item in enumerate(news_items, 1):  # ëª¨ë“  ë‰´ìŠ¤ ë¶„ì„
                logger.info(f"ë‰´ìŠ¤ {i}/{len(news_items)} ë¶„ì„ ì¤‘: {news_item.get('headline', '')[:50]}...")
                headline = news_item.get('headline', '')
                url = news_item.get('url', '')
                
                if not headline:
                    continue
                
                try:
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": SYSTEM_PROMPT_ANALYSIS},
                            {"role": "user", "content": headline}
                        ],
                        max_tokens=100,
                        temperature=0.1
                    )
                    
                    result_text = response.choices[0].message.content.strip()
                    
                    # ê²°ê³¼ íŒŒì‹±
                    if "|" in result_text:
                        parts = result_text.split("|")
                        if len(parts) >= 3:
                            results.append({
                                'headline': parts[0].strip(),
                                'sentiment': parts[1].strip(),
                                'reason': parts[2].strip(),
                                'url': url,
                                'score': 0.8,
                                'confidence': 0.9
                            })
                
                except openai.RateLimitError:
                    logger.warning("OpenAI API rate limit ë„ë‹¬")
                    break
                except openai.APIError as e:
                    logger.error(f"OpenAI API ì˜¤ë¥˜: {e}")
                    continue
                except Exception as e:
                    logger.error(f"ê°œë³„ ë‰´ìŠ¤ ê°ì •ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê°ì •ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def generate_level_summary(self, news_items: List[Dict], level: int, api_key: str = None, mpti_type: str = 'Fact') -> str:
        """ë ˆë²¨ë³„ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± (MPTI ìŠ¤íƒ€ì¼ ì ìš©)"""
        if not news_items:
            return "ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        if not api_key:
            import os
            api_key = os.getenv('OPENAI_API_KEY')
        
        if not api_key:
            return "OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        try:
            # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ì¶”ì¶œ
            headlines = [item.get('headline', '') for item in news_items if item.get('headline')]
            if not headlines:
                return "ë¶„ì„í•  ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤."
            
            news_summary = " ".join(headlines)
            
            client = openai.OpenAI(api_key=api_key)
            level_prompt = LEVEL_PROMPTS.get(level, LEVEL_PROMPTS[3])
            
            # MPTI ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
            try:
                from chatbot.config import Config
                mpti_styles = Config.MPTI_STYLES
                mpti_prompt = mpti_styles.get(mpti_type, {}).get('prompt', '')
            except ImportError:
                mpti_prompt = ""
            
            # ë ˆë²¨ê³¼ MPTI ìŠ¤íƒ€ì¼ì„ ê²°í•©í•œ í”„ë¡¬í”„íŠ¸
            combined_prompt = f"{level_prompt}"
            if mpti_prompt:
                combined_prompt += f" {mpti_prompt}"
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": f"ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {combined_prompt}"},
                    {"role": "user", "content": f"ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•´ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”: {news_summary}"}
                ],
                max_tokens=200,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except openai.RateLimitError:
            logger.warning("OpenAI API rate limit ë„ë‹¬")
            return "API ì‚¬ìš©ëŸ‰ í•œê³„ë¡œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except openai.APIError as e:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {e}")
            return "API ì˜¤ë¥˜ë¡œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"ë ˆë²¨ë³„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def display_news_analysis(self, code: str, level: int, mpti_type: str):
        """ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
        st.markdown(f'<div class="section-header">ğŸ“° ë‰´ìŠ¤ ê°ì •ë¶„ì„ <span class="level-indicator">Level {level}</span></div>', unsafe_allow_html=True)
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not code or not isinstance(code, str):
                st.error("ìœ íš¨í•˜ì§€ ì•Šì€ ì½”ë“œì…ë‹ˆë‹¤.")
                return
            
            # ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            with st.spinner("ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                news_items = self.fetch_naver_news(code)
            
            if news_items:
                # ê°ì •ë¶„ì„ ìˆ˜í–‰
                with st.spinner("ë‰´ìŠ¤ ê°ì •ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    sentiment_results = self.analyze_news_sentiment(news_items)
                
                # ë‰´ìŠ¤ëŠ” ìˆì§€ë§Œ ê°ì •ë¶„ì„ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ê¸°ë³¸ ì •ë³´ í‘œì‹œ
                if sentiment_results:
                    # ê°ì • ë¶„í¬ ê³„ì‚°
                    sentiment_counts = {}
                    for result in sentiment_results:
                        sentiment = result.get('sentiment', '')
                        if sentiment:
                            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
                    
                    # ê°ì • ë¶„í¬ ì„¹ì…˜
                    st.markdown("### ğŸ“Š ê°ì • ë¶„í¬")
                    
                    # ê°ì •ë³„ ì¹´ë“œ
                    if sentiment_counts:
                        cols = st.columns(len(sentiment_counts))
                    sentiment_config = {
                        'ê¸ì •': {'color': '#28a745', 'icon': 'ğŸ˜Š', 'bg_color': '#d4edda'},
                        'ë¶€ì •': {'color': '#dc3545', 'icon': 'ğŸ˜', 'bg_color': '#f8d7da'},
                        'ì¤‘ë¦½': {'color': '#6c757d', 'icon': 'ğŸ˜', 'bg_color': '#e2e3e5'}
                    }
                    
                    for i, (sentiment, count) in enumerate(sentiment_counts.items()):
                        with cols[i]:
                            config = sentiment_config.get(sentiment, {'color': '#6c757d', 'icon': 'âšª', 'bg_color': '#f8f9fa'})
                            st.markdown(f"""
                            <div style="
                                background: {config['bg_color']};
                                border: 2px solid {config['color']};
                                border-radius: 10px;
                                padding: 1rem;
                                text-align: center;
                                margin: 0.5rem 0;">
                                <h3 style="color: {config['color']}; margin: 0;">{config['icon']} {sentiment}</h3>
                                <h2 style="color: {config['color']}; margin: 0.5rem 0;">{count}</h2>
                                <p style="margin: 0; color: #666;">ê°œ ë‰´ìŠ¤</p>
                            </div>
                            """, unsafe_allow_html=True)
                    
                    # íŒŒì´ ì°¨íŠ¸
                    if len(sentiment_counts) > 1:
                        try:
                            import plotly.graph_objects as go
                            
                            colors = [sentiment_config.get(s, {}).get('color', '#6c757d') for s in sentiment_counts.keys()]
                            
                            fig = go.Figure(data=[go.Pie(
                                labels=list(sentiment_counts.keys()),
                                values=list(sentiment_counts.values()),
                                hole=0.4,
                                marker_colors=colors,
                                textinfo='label+percent',
                                textfont_size=14
                            )])
                            
                            fig.update_layout(
                                title="ë‰´ìŠ¤ ê°ì • ë¶„í¬",
                                showlegend=True,
                                height=400,
                                margin=dict(t=50, b=50, l=50, r=50)
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                        except Exception as e:
                            logger.error(f"ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                            st.warning("ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ìƒì„¸ ë¶„ì„ ê²°ê³¼
                    st.markdown("### ğŸ“ ìƒì„¸ ë¶„ì„ ê²°ê³¼")
                    
                    for i, result in enumerate(sentiment_results, 1):
                        sentiment = result.get('sentiment', '')
                        config = sentiment_config.get(sentiment, {'color': '#6c757d', 'icon': 'âšª', 'bg_color': '#f8f9fa'})
                        url = result.get('url', '')
                        
                        # ë‰´ìŠ¤ ì¹´ë“œ
                        st.markdown(f"""
                        <div style="
                            background: {config['bg_color']};
                            border-left: 5px solid {config['color']};
                            border-radius: 8px;
                            padding: 1.5rem;
                            margin: 1rem 0;
                            box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                            <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
                                <span style="font-size: 1.5rem; margin-right: 0.5rem;">{config['icon']}</span>
                                <h4 style="margin: 0; color: {config['color']};">{sentiment}</h4>
                            </div>
                            <h5 style="margin: 0.5rem 0; color: #333;">{result.get('headline', '')}</h5>
                            <p style="margin: 0.5rem 0; color: #666; font-size: 0.9rem;">ğŸ’¡ {result.get('reason', '')}</p>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # ë§í¬ ë²„íŠ¼
                        if url and self._is_valid_url(url):
                            if st.button(f"ğŸ”— ë‰´ìŠ¤ ë³´ê¸° ({i})", key=f"news_link_{i}"):
                                st.markdown(f"[ë‰´ìŠ¤ ì›ë¬¸ ë³´ê¸°]({url})")
                    
                    # ë ˆë²¨ë³„ ìš”ì•½
                    summary = self.generate_level_summary(news_items, level)
                    if summary:
                        st.markdown("### ğŸ“‹ ì¢…í•© ìš”ì•½")
                        st.markdown(f"""
                        <div style="
                            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                            color: white;
                            padding: 1.5rem;
                            border-radius: 10px;
                            margin: 1rem 0;">
                            <h4 style="margin: 0 0 1rem 0;">ğŸ¯ AI ë¶„ì„ ìš”ì•½</h4>
                            <p style="margin: 0; line-height: 1.6;">{summary}</p>
                        </div>
                        """, unsafe_allow_html=True)
                
                else:
                    # ë‰´ìŠ¤ëŠ” ìˆì§€ë§Œ ê°ì •ë¶„ì„ì´ ì‹¤íŒ¨í•œ ê²½ìš°, ê¸°ë³¸ ë‰´ìŠ¤ ì •ë³´ í‘œì‹œ
                    st.warning("ë‰´ìŠ¤ ê°ì •ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ê¸°ë³¸ ë‰´ìŠ¤ ëª©ë¡ í‘œì‹œ
                    st.markdown("### ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤")
                    for i, news in enumerate(news_items, 1):
                        st.markdown(f"""
                        <div style="
                            background: white;
                            border-left: 5px solid #FFD700;
                            border-radius: 8px;
                            padding: 1.5rem;
                            margin: 1rem 0;
                            box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                            <h5 style="margin: 0.5rem 0; color: #333;">{i}. {news.get('headline', '')}</h5>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # ë§í¬ ë²„íŠ¼
                        url = news.get('url', '')
                        if url and self._is_valid_url(url):
                            if st.button(f"ğŸ”— ë‰´ìŠ¤ ë³´ê¸° ({i})", key=f"basic_news_link_{i}"):
                                st.markdown(f"[ë‰´ìŠ¤ ì›ë¬¸ ë³´ê¸°]({url})")
            else:
                st.info("ìµœê·¼ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            st.error(f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜: {e}")

