"""
ë‰´ìŠ¤ ë¶„ì„ ëª¨ë“ˆ
- ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° GPT ê°ì •ë¶„ì„
- ë ˆë²¨ë³„ ë§ì¶¤í˜• ìš”ì•½
- í•´ì™¸ ì¢…ëª©/í‚¤ì›Œë“œ ì§€ì›
"""

import streamlit as st
import pandas as pd
import numpy as np
import requests
from datetime import datetime, timedelta
import logging
from typing import Dict, Any, Optional, List, Tuple
import time
import re
import random
from bs4 import BeautifulSoup
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import openai
from urllib.parse import urlparse
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ Python ê²½ë¡œì— ì¶”ê°€
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì„¤ì • í´ë˜ìŠ¤
class Config:
    NAVER_FINANCE_BASE_URL = "https://finance.naver.com"

    LEVEL_PROMPTS = {
        1: """- Level 1 (ì´ˆë³´ì): 
        â€¢ ì–´íˆ¬: ìœ ì¹˜ì›/ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ì•„ì£¼ ì‰¬ìš´ ë§ë¡œ ì„¤ëª…
        â€¢ ë‚´ìš©: íˆ¬ì ê¸°ì´ˆ ê°œë… ìœ„ì£¼, ë³µì¡í•œ ìš©ì–´ëŠ” ë¹„ìœ ì™€ ì˜ˆì‹œë¡œ ëŒ€ì²´
        â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ í•µì‹¬ë§Œ ìš”ì•½""",
        
        2: """- Level 2 (ì…ë¬¸ì): 
        â€¢ ì–´íˆ¬: ì¤‘ê³ ë“±í•™ìƒë„ ì´í•´ ê°€ëŠ¥í•œ ì‰¬ìš´ ë§ë¡œ ì„¤ëª…
        â€¢ ë‚´ìš©: í•µì‹¬ ê°œë…ê³¼ ì´ìœ ë¥¼ í¬í•¨, ê¸°ë³¸ì ì¸ íˆ¬ì ì§€ì‹ ì „ë‹¬
        â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ì„¤ëª…""",
        
        3: """- Level 3 (ì¤‘ê¸‰ì): 
        â€¢ ì–´íˆ¬: ì¼ë°˜ ì„±ì¸ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…
        â€¢ ë‚´ìš©: ì‹¤ì „ íŒê³¼ êµ¬ì²´ì  ì „ëµ í¬í•¨, ë°ì´í„° ê¸°ë°˜ ë¶„ì„
        â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ë¶„ì„""",
        
        4: """- Level 4 (ê³ ê¸‰ì): 
        â€¢ ì–´íˆ¬: íˆ¬ì ê²½í—˜ì´ ìˆëŠ” ì„±ì¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì „ë¬¸ì  ì„¤ëª…
        â€¢ ë‚´ìš©: ì‹¬í™” ë¶„ì„ê³¼ ê³ ê¸‰ ì „ëµ, ì‹œì¥ ë™í–¥ê³¼ ì—°ê´€ì„± ë¶„ì„
        â€¢ ê¸¸ì´: 1-2ì¤„ë¡œ ìƒì„¸ ì„¤ëª…""",
        
        5: """- Level 5 (ì „ë¬¸ê°€): 
        â€¢ ì–´íˆ¬: íˆ¬ì ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ê³ ê¸‰ ë¶„ì„ê³¼ ì „ë¬¸ ìš©ì–´ ì‚¬ìš©
        â€¢ ë‚´ìš©: ìµœê³  ìˆ˜ì¤€ ë¶„ì„ê³¼ ì‹¤ì „ í™œìš©, ì‹œì¥ ë¯¸ì‹œêµ¬ì¡°ê¹Œì§€ ê³ ë ¤
        â€¢ ê¸¸ì´: 1-2ì¤„ ì´ìƒìœ¼ë¡œ ì „ë¬¸ì  ì„¤ëª…"""
    }

# ì„¤ì • ê°ì²´
try:
    from chatbot.config import Config
    config = Config()
except ImportError:
    config = Config()

SYSTEM_PROMPT_ANALYSIS = """
- ë‹¹ì‹ ì€ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ê°ì„± ë¶„ì„ê¸°ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ ë³´ë‚¸ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë³´ê³ , ë‹¤ìŒ 3ê°œì˜ í•„ë“œë§Œ "|" ë¡œ êµ¬ë¶„í•´ í•œ ì¤„ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
  1) ë‰´ìŠ¤ê¸°ì‚¬(ì›ë¬¸ ê·¸ëŒ€ë¡œ)
  2) ê¸ë¶€ì • ê²°ê³¼ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜)
  3) ì´ìœ  (í•œ ë¬¸ì¥)
- ë°˜ë“œì‹œ ìœ„ ìˆœì„œëŒ€ë¡œ "ë‰´ìŠ¤ê¸°ì‚¬|ê¸ë¶€ì • ê²°ê³¼|ì´ìœ " í˜•íƒœë¡œë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.
"""

class NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ í´ë˜ìŠ¤ - ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì „ìš©"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.timeout = 10
        
        # ì„¤ì • ê°ì²´
        self.config = config
        
        # User-Agent ëœë¤í™”
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        selected_agent = random.choice(user_agents)
        
        # í—¤ë” ì„¤ì • (ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ìš©)
        self.headers = {
            'User-Agent': selected_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': f'{self.config.NAVER_FINANCE_BASE_URL}/',
        }
        
        # í—ˆìš©ëœ ë„ë©”ì¸ (ë„¤ì´ë²„ ê¸ˆìœµë§Œ)
        self.allowed_domains = ['finance.naver.com', 'search.naver.com']
    
    def _is_valid_url(self, url: str) -> bool:
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            parsed = urlparse(url)
            return parsed.netloc in self.allowed_domains
        except:
            return False
    
    def _sanitize_url(self, url: str) -> str:
        """URL ì •ê·œí™” ë° ë³´ì•ˆ ê²€ì‚¬"""
        if not url:
            return ""
        
        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
        if url.startswith('/'):
            url = f"https://finance.naver.com{url}"
        
        # URL ìœ íš¨ì„± ê²€ì‚¬
        if not self._is_valid_url(url):
            return ""
        
        return url
    
    def _is_relevant_news(self, headline: str, keyword: str) -> bool:
        """ë‰´ìŠ¤ ê´€ë ¨ì„± ê²€ì‚¬"""
        if not headline or not keyword:
            return False
        
        headline_lower = headline.lower()
        keyword_lower = keyword.lower()
        
        # í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        return keyword_lower in headline_lower
    
    def _is_relevant_news_relaxed(self, headline: str, keyword: str) -> bool:
        """ë‰´ìŠ¤ ê´€ë ¨ì„± ê²€ì‚¬ (ë” ê´€ëŒ€í•˜ê²Œ)"""
        if not headline or not keyword:
            return False
        
        headline_lower = headline.lower()
        keyword_lower = keyword.lower()
        
        # 1. ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
        if keyword_lower in headline_lower:
            return True
        
        # 2. í•´ì™¸ ì¢…ëª©ë³„ ê³ ìœ  í‚¤ì›Œë“œ ë§¤í•‘ (ì¤‘ë³µ ë°©ì§€)
        overseas_keywords = {
            'nvidia': ['ì—”ë¹„ë””ì•„', 'nvidia', 'gpu', 'ai ë°˜ë„ì²´'],
            'ì—”ë¹„ë””ì•„': ['ì—”ë¹„ë””ì•„', 'nvidia', 'gpu', 'ai ë°˜ë„ì²´'],
            'amd': ['amd', 'ë¼ì´ì  ', 'amd ë°˜ë„ì²´'],
            'intel': ['ì¸í…”', 'intel', 'cpu', 'ì¸í…” ë°˜ë„ì²´'],
            'ì¸í…”': ['ì¸í…”', 'intel', 'cpu', 'ì¸í…” ë°˜ë„ì²´'],
            'í€„ì»´': ['í€„ì»´', 'qualcomm', 'ëª¨ë°”ì¼', '5g'],
            'ë¸Œë¡œë“œì»´': ['ë¸Œë¡œë“œì»´', 'broadcom', 'ë„¤íŠ¸ì›Œí¬']
        }
        
        # í•´ë‹¹ ì¢…ëª©ì˜ ê³ ìœ  í‚¤ì›Œë“œë§Œ í™•ì¸
        if keyword_lower in overseas_keywords:
            for related_keyword in overseas_keywords[keyword_lower]:
                if related_keyword.lower() in headline_lower:
                    return True
        
        # 3. ì¢…ëª©ëª… + ì£¼ê°€/ì£¼ì‹ í‚¤ì›Œë“œ ì¡°í•© (ë” ì •í™•í•œ ë§¤ì¹­)
        stock_price_keywords = ['ì£¼ê°€', 'ì£¼ì‹', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ìƒìŠ¹', 'í•˜ë½', 'ê¸‰ë“±', 'ê¸‰ë½']
        for price_keyword in stock_price_keywords:
            if price_keyword in headline_lower and keyword_lower in headline_lower:
                return True
        
        # 4. ì¼ë°˜ì ì¸ ê¸ˆìœµ í‚¤ì›Œë“œ (ì¢…ëª©ëª…ì´ í¬í•¨ëœ ê²½ìš°ë§Œ)
        if keyword_lower in headline_lower:
            finance_keywords = ['íˆ¬ì', 'ì¦ì‹œ', 'í€ë“œ', 'etf', 'ì±„ê¶Œ', 'ê¸ˆë¦¬', 'ê²½ì œ']
            for finance_keyword in finance_keywords:
                if finance_keyword in headline_lower:
                    return True
        
        # 5. ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ (ì¢…ëª©ëª…ì´ í¬í•¨ëœ ê²½ìš°ë§Œ)
        if keyword_lower in headline_lower:
            tech_keywords = ['ë°˜ë„ì²´', 'ì¹©', 'ë©”ëª¨ë¦¬', 'ai', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹']
            for tech_keyword in tech_keywords:
                if tech_keyword in headline_lower:
                    return True
        
        # 6. í•´ì™¸ ì‹œì¥ ê´€ë ¨ í‚¤ì›Œë“œ (ì¢…ëª©ëª…ì´ í¬í•¨ëœ ê²½ìš°ë§Œ)
        if keyword_lower in headline_lower:
            market_keywords = ['ë‚˜ìŠ¤ë‹¥', 'nasdaq', 'ë‹¤ìš°', 'dow', 'ì›”ê°€', 'wall street', 'ë¯¸êµ­']
            for market_keyword in market_keywords:
                if market_keyword in headline_lower:
                    return True
        
        return False
    
    def _is_relevant_news_strict(self, headline: str, keyword: str) -> bool:
        """ë‰´ìŠ¤ ê´€ë ¨ì„± ê²€ì‚¬ (ë§¤ìš° ì—„ê²©í•œ ë²„ì „)"""
        if not headline or not keyword:
            return False
        
        headline_lower = headline.lower()
        keyword_lower = keyword.lower()
        
        # 1. ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¥ ìš°ì„ )
        if keyword_lower in headline_lower:
            return True
        
        # 2. í•´ì™¸ ì¢…ëª©ë³„ ê³ ìœ  í‚¤ì›Œë“œ ë§¤í•‘ (ë” êµ¬ì²´ì ìœ¼ë¡œ)
        stock_keywords = {
            'nvidia': ['ì—”ë¹„ë””ì•„', 'nvidia', 'gpu', 'ai ë°˜ë„ì²´', 'h100', 'a100', 'ë°ì´í„°ì„¼í„°', 'ì—”ë¹„ë””ì•„ ì£¼ê°€'],
            'ì—”ë¹„ë””ì•„': ['ì—”ë¹„ë””ì•„', 'nvidia', 'gpu', 'ai ë°˜ë„ì²´', 'h100', 'a100', 'ë°ì´í„°ì„¼í„°', 'ì—”ë¹„ë””ì•„ ì£¼ê°€'],
            'nvda': ['ì—”ë¹„ë””ì•„', 'nvidia', 'gpu', 'ai ë°˜ë„ì²´', 'h100', 'a100', 'ë°ì´í„°ì„¼í„°', 'ì—”ë¹„ë””ì•„ ì£¼ê°€'],
            'amd': ['amd', 'ë¼ì´ì  ', 'ryzen', 'epyc', 'amd ë°˜ë„ì²´', 'zen', 'amd ì£¼ê°€'],
            'intel': ['ì¸í…”', 'intel', 'cpu', 'ì¸í…” ë°˜ë„ì²´', 'idm', 'foundry', 'ì¸í…” ì£¼ê°€'],
            'ì¸í…”': ['ì¸í…”', 'intel', 'cpu', 'ì¸í…” ë°˜ë„ì²´', 'idm', 'foundry', 'ì¸í…” ì£¼ê°€'],
            'intc': ['ì¸í…”', 'intel', 'cpu', 'ì¸í…” ë°˜ë„ì²´', 'idm', 'foundry', 'ì¸í…” ì£¼ê°€'],
            'í€„ì»´': ['qualcomm', 'í€„ì»´', 'ìŠ¤ëƒ…ë“œë˜ê³¤', 'snapdragon', 'ëª¨ë°”ì¼', '5g', 'í€„ì»´ ì£¼ê°€'],
            'ë¸Œë¡œë“œì»´': ['ë¸Œë¡œë“œì»´', 'broadcom', 'ë„¤íŠ¸ì›Œí¬', 'network', 'ë¸Œë¡œë“œì»´ ì£¼ê°€'],
            '005930': ['ì‚¼ì„±ì „ì', 'ë©”ëª¨ë¦¬', 'dram', 'nand', 'ê°¤ëŸ­ì‹œ', 'galaxy', 'ì‚¼ì„±ì „ì ì£¼ê°€'],
            '000660': ['skí•˜ì´ë‹‰ìŠ¤', 'hbm', 'ë©”ëª¨ë¦¬', 'dram', 'nand', 'skí•˜ì´ë‹‰ìŠ¤ ì£¼ê°€']
        }
        
        # í•´ë‹¹ ì¢…ëª©ì˜ ê³ ìœ  í‚¤ì›Œë“œë§Œ í™•ì¸
        if keyword_lower in stock_keywords:
            for related_keyword in stock_keywords[keyword_lower]:
                if related_keyword.lower() in headline_lower:
                    return True
        
        # 3. ì¢…ëª©ëª… + ì£¼ê°€/ì£¼ì‹ í‚¤ì›Œë“œ ì¡°í•© (ì •í™•í•œ ë§¤ì¹­)
        stock_price_keywords = ['ì£¼ê°€', 'ì£¼ì‹', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ìƒìŠ¹', 'í•˜ë½', 'ê¸‰ë“±', 'ê¸‰ë½', 'íˆ¬ì', 'ì¦ì‹œ']
        for price_keyword in stock_price_keywords:
            if price_keyword in headline_lower and keyword_lower in headline_lower:
                return True
        
        # 4. ì¢…ëª©ëª… + ê¸°ìˆ  í‚¤ì›Œë“œ ì¡°í•© (ì •í™•í•œ ë§¤ì¹­)
        tech_keywords = ['ë°˜ë„ì²´', 'ì¹©', 'ë©”ëª¨ë¦¬', 'ai', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'gpu', 'cpu', 'ë°ì´í„°ì„¼í„°']
        for tech_keyword in tech_keywords:
            if tech_keyword in headline_lower and keyword_lower in headline_lower:
                return True
        
        # 5. ì¢…ëª©ëª… + ì‹œì¥ í‚¤ì›Œë“œ ì¡°í•© (ì •í™•í•œ ë§¤ì¹­)
        market_keywords = ['ë‚˜ìŠ¤ë‹¥', 'nasdaq', 'ë‹¤ìš°', 'dow', 'ì›”ê°€', 'wall street', 'ë¯¸êµ­', 'ì¦ì‹œ', 'ì£¼ì‹ì‹œì¥']
        for market_keyword in market_keywords:
            if market_keyword in headline_lower and keyword_lower in headline_lower:
                return True
        
        # 6. ì¼ë°˜ì ì¸ ê¸ˆìœµ/íˆ¬ì í‚¤ì›Œë“œëŠ” ì œì™¸ (ë„ˆë¬´ ê´‘ë²”ìœ„í•¨)
        general_finance_words = ['ì£¼ì‹', 'íˆ¬ì', 'ì‹œì¥', 'ê²½ì œ', 'ê¸ˆìœµ', 'ì€í–‰', 'ì¦ê¶Œ', 'ì—°ì²´ê¸ˆ', 'ì‹ ìš©ì‚¬ë©´', 'ë¹š', '324ë§Œëª…', '5000ë§Œì›']
        if any(word in headline_lower for word in general_finance_words):
            # ì¼ë°˜ì ì¸ ê¸ˆìœµ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì¢…ëª©ëª…ë„ í•¨ê»˜ ìˆì–´ì•¼ í•¨
            if keyword_lower not in headline_lower:
                return False
        
        # 7. íŠ¹ì • ì œì™¸ í‚¤ì›Œë“œ (ê´€ë ¨ ì—†ëŠ” ë‰´ìŠ¤)
        exclude_keywords = ['ì—°ì²´ê¸ˆ', 'ì‹ ìš©ì‚¬ë©´', '324ë§Œëª…', '5000ë§Œì›', 'ë¹š', 'ì—°ë‚´', 'ê°šìœ¼ë©´']
        if any(word in headline_lower for word in exclude_keywords):
            # ì œì™¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì¢…ëª©ëª…ë„ í•¨ê»˜ ìˆì–´ì•¼ í•¨
            if keyword_lower not in headline_lower:
                return False
        
        return False
    
    def _search_naver_finance_news(self, keyword: str) -> List[Dict]:
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ (êµ­ë‚´/í•´ì™¸ ì¢…ëª© ëª¨ë‘ ì§€ì›)"""
        try:
            # í•´ì™¸ ì¢…ëª© ë§¤í•‘ (ì˜ì–´ëª… â†’ í•œêµ­ì–´ëª…)
            overseas_stocks = {
                'NVDA': 'ì—”ë¹„ë””ì•„',
                'AMD': 'AMD',
                'INTC': 'ì¸í…”',
                'QCOM': 'í€„ì»´',
                'AVGO': 'ë¸Œë¡œë“œì»´',
                'AAPL': 'ì• í”Œ',
                'MSFT': 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸',
                'GOOGL': 'ì•ŒíŒŒë²³',
                'AMZN': 'ì•„ë§ˆì¡´',
                'TSLA': 'í…ŒìŠ¬ë¼',
                'META': 'ë©”íƒ€',
                'NFLX': 'ë„·í”Œë¦­ìŠ¤',
                'TSM': 'TSMC',
                'ASML': 'ASML',
                'SMIC': 'SMIC'
            }
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            search_keywords = [keyword]
            
            # í•´ì™¸ ì¢…ëª©ì¸ ê²½ìš° í•œêµ­ì–´ ì´ë¦„ë„ ì¶”ê°€
            if keyword.upper() in overseas_stocks:
                korean_name = overseas_stocks[keyword.upper()]
                search_keywords.append(korean_name)
                logger.info(f"í•´ì™¸ ì¢…ëª© ë³€í™˜: {keyword} â†’ {korean_name}")
            elif keyword in overseas_stocks:
                korean_name = overseas_stocks[keyword]
                search_keywords.append(korean_name)
                logger.info(f"í•´ì™¸ ì¢…ëª© ë³€í™˜: {keyword} â†’ {korean_name}")
            
            # ì˜ì–´ëª…ë„ ì¶”ê°€ (í•œêµ­ì–´ëª…ì´ ì…ë ¥ëœ ê²½ìš°)
            reverse_mapping = {v: k for k, v in overseas_stocks.items()}
            if keyword in reverse_mapping:
                english_name = reverse_mapping[keyword]
                search_keywords.append(english_name)
                logger.info(f"í•œêµ­ì–´ëª… ë³€í™˜: {keyword} â†’ {english_name}")
            
            logger.info(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
            
            # 1. ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª©ë³„ ë‰´ìŠ¤ (êµ­ë‚´ ì¢…ëª©ì½”ë“œì¸ ê²½ìš°)
            if keyword.isdigit() and len(keyword) == 6:
                try:
                    url = f"{self.config.NAVER_FINANCE_BASE_URL}/item/news_news.naver?code={keyword}"
                    headers = {
                        "User-Agent": "Mozilla/5.0",
                        "Referer": url
                    }
                    
                    resp = self.session.get(url, headers=headers, timeout=self.timeout)
                    resp.raise_for_status()
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    news_items = []
                    
                    # ë‰´ìŠ¤ ìˆ˜ì§‘
                    for row in soup.select("table.type5 tbody tr"):
                        title_tag = row.select_one("td.title a.tit")
                        date_tag = row.select_one("td.date")
                        
                        if not title_tag or not date_tag:
                            continue
                        
                        try:
                            # ë‚ ì§œ íŒŒì‹±
                            date_str = date_tag.text.strip()
                            dt = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
                            
                            # 14ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ
                            if dt < datetime.now() - timedelta(days=14):
                                continue
                            
                            headline = title_tag.text.strip()
                            
                            # URL ìƒì„± 
                            href = title_tag.get('href', '')
                            if href.startswith('/'):
                                href = f"{self.config.NAVER_FINANCE_BASE_URL}{href}"
                            
                            news_items.append({
                                'headline': headline,
                                'url': href,
                                'date': dt
                            })
                        
                        except ValueError:
                            continue
                    
                    # ë‚ ì§œìˆœ ì •ë ¬ í›„ ìµœê·¼ 3ê°œ ë°˜í™˜
                    news_items.sort(key=lambda x: x['date'], reverse=True)
                    result = []
                    for item in news_items[:3]:
                        result.append({
                            'headline': item['headline'],
                            'url': item['url']
                        })
                    
                    if result:
                        logger.info(f"ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ: {len(result)}ê°œ")
                        return result
                
                except Exception as e:
                    logger.debug(f"ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
            
            # 2. í•´ì™¸ ì¢…ëª©/í‚¤ì›Œë“œ ë‰´ìŠ¤ ê²€ìƒ‰ (ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ì‹œë„)
            for search_keyword in search_keywords:
                try:
                    overseas_news = self._search_naver_finance_news_alt(search_keyword)
                    if overseas_news:
                        # ê´€ë ¨ì„± í•„í„°ë§ ê°•í™”
                        filtered_news = []
                        for news in overseas_news:
                            if self._is_relevant_news_strict(news['headline'], keyword):
                                filtered_news.append(news)
                            elif len(filtered_news) < 1:  # ê´€ë ¨ì„±ì´ ë‚®ì€ ë‰´ìŠ¤ëŠ” ìµœëŒ€ 1ê°œê¹Œì§€ë§Œ
                                filtered_news.append(news)
                        
                        if filtered_news:
                            logger.info(f"í‚¤ì›Œë“œ '{search_keyword}'ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ: {len(filtered_news)}ê°œ")
                            return filtered_news
                except Exception as e:
                    logger.debug(f"í‚¤ì›Œë“œ '{search_keyword}' ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.warning(f"ëª¨ë“  í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {search_keywords}")
            return []
            
        except Exception as e:
            logger.debug(f"ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
        return []
    
    def _search_naver_finance_news_alt(self, keyword: str) -> List[Dict]:
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ (ëŒ€ì•ˆ ë°©ë²•)"""
        try:
            encoded_keyword = requests.utils.quote(keyword)
            # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ URL ì‚¬ìš©
            url = f"https://finance.naver.com/news/news_search.naver?query={encoded_keyword}&searchType=0&page=1"
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Referer": f"{self.config.NAVER_FINANCE_BASE_URL}/"
            }
            
            resp = self.session.get(url, headers=headers, timeout=self.timeout)
            resp.raise_for_status()
            
            soup = BeautifulSoup(resp.text, "html.parser")
            news_items = []
            
            # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
            news_selectors = [
                "table.type06 a[href*='news_read']",
                "table.type5 a[href*='news_read']",
                ".news_list a[href*='news_read']",
                "a[href*='news_read']"
            ]
            
            for selector in news_selectors:
                news_links = soup.select(selector)
                
                for link in news_links[:10]:
                    try:
                        headline = link.text.strip()
                        if not headline or len(headline) < 5:
                            continue
                        
                        href = link.get('href', '')
                        if not href:
                            continue
                        
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if href.startswith('/'):
                            href = f"{self.config.NAVER_FINANCE_BASE_URL}{href}"
                        
                        # ê´€ë ¨ì„± ì²´í¬ (ì¢…ëª©ë³„ ê³ ìœ  ë‰´ìŠ¤ ìš°ì„ )
                        if self._is_relevant_news_strict(headline, keyword):
                            news_items.append({
                                'headline': headline,
                                'url': href
                            })
                        # ê´€ë ¨ì„±ì´ ë‚®ì€ ë‰´ìŠ¤ëŠ” ë‰´ìŠ¤ê°€ ë¶€ì¡±í•œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                        elif len(news_items) < 1:
                            news_items.append({
                                'headline': headline,
                                'url': href
                            })
                        
                        if len(news_items) >= 3:
                            break
                            
                    except Exception as e:
                        continue
                
                if len(news_items) >= 3:
                    break
            
            logger.info(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            return news_items
            
        except Exception as e:
            logger.debug(f"ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
            return []
    
    def _search_naver_general_news(self, keyword: str) -> List[Dict]:
        """ë„¤ì´ë²„ ì¼ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ (ì°¸ê³  ì½”ë“œ ê¸°ë°˜)"""
        try:
            encoded_keyword = requests.utils.quote(keyword)
            # ë„¤ì´ë²„ ì¼ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ URL ì‚¬ìš©
            url = f'https://search.naver.com/search.naver?where=news&query={encoded_keyword}'
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1"
            }
            
            resp = self.session.get(url, headers=headers, timeout=self.timeout)
            resp.raise_for_status()
            
            soup = BeautifulSoup(resp.text, "html.parser")
            news_items = []
            
            # ë„¤ì´ë²„ ì¼ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬ ì°¾ê¸°
            # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
            news_selectors = [
                '.news_tit',
                '.group_news a',
                '.group_news .news_tit',
                '.news_area a',
                '.news_area .news_tit',
                'a[href*="news.naver.com"]',
                '.group_news a[href*="news.naver.com"]'
            ]
            
            news_links = []
            for selector in news_selectors:
                links = soup.select(selector)
                if links:
                    news_links = links
                    logger.debug(f"ë„¤ì´ë²„ ì¼ë°˜ ë‰´ìŠ¤ ì„ íƒì ì„±ê³µ: {selector}")
                    break
            
            for link in news_links[:10]:  # ìµœëŒ€ 10ê°œ ë‰´ìŠ¤ì—ì„œ í•„í„°ë§
                try:
                    headline = link.text.strip()
                    if not headline or len(headline) < 5:
                        continue
                    
                    href = link.get('href', '')
                    if not href:
                        continue
                    
                    # ê´€ë ¨ì„± ì²´í¬ (ë§¤ìš° ì—„ê²©í•˜ê²Œ)
                    if self._is_relevant_news_strict(headline, keyword):
                        news_items.append({
                            'headline': headline,
                            'url': href
                        })
                    # ê´€ë ¨ì„±ì´ ë‚®ì€ ë‰´ìŠ¤ëŠ” ì œì™¸ (ì¼ë°˜ ë‰´ìŠ¤ëŠ” ë” ì—„ê²©í•˜ê²Œ)
                    
                    if len(news_items) >= 3:  # ìµœëŒ€ 3ê°œ ë‰´ìŠ¤
                        break
                        
                except Exception as e:
                    logger.debug(f"ë‰´ìŠ¤ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"ë„¤ì´ë²„ ì¼ë°˜ ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ '{keyword}'ë¡œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            return news_items
            
        except Exception as e:
            logger.debug(f"ë„¤ì´ë²„ ì¼ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
            return []
                
   
    
    def _get_stock_name(self, code: str) -> str:
        """ì¢…ëª©ì½”ë“œë¡œ ì¢…ëª©ëª… ê°€ì ¸ì˜¤ê¸°"""
        stock_name_mapping = {
            # êµ­ë‚´ ì¢…ëª©
            '005930': 'ì‚¼ì„±ì „ì',
            '000660': 'SKí•˜ì´ë‹‰ìŠ¤',
            '042700': 'í•œë¯¸ë°˜ë„ì²´',
            '035420': 'NAVER',
            '035720': 'ì¹´ì¹´ì˜¤',
            '373220': 'LGì—ë„ˆì§€ì†”ë£¨ì…˜',
            '207940': 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤',
            '005380': 'í˜„ëŒ€ì°¨',
            '000270': 'ê¸°ì•„',
            '005490': 'POSCOí™€ë”©ìŠ¤',
            '051910': 'LGí™”í•™',
            '006400': 'ì‚¼ì„±SDI',
            '096770': 'SKì´ë…¸ë² ì´ì…˜',
            '066570': 'LGì „ì',
            '032830': 'ì‚¼ì„±ìƒëª…',
            '105560': 'KBê¸ˆìœµ',
            '055550': 'ì‹ í•œì§€ì£¼',
            '086790': 'í•˜ë‚˜ê¸ˆìœµì§€ì£¼',
            # í•´ì™¸ ì¢…ëª© (í‹°ì»¤ â†’ í•œêµ­ì–´ëª…)
            'NVDA': 'NVIDIA',
            'AMD': 'AMD',
            'INTC': 'Intel',
            'QCOM': 'Qualcomm',
            'AVGO': 'Broadcom',
            'AAPL': 'Apple',
            'MSFT': 'Microsoft',
            'GOOGL': 'Alphabet',
            'AMZN': 'Amazon',
            'TSLA': 'Tesla',
            'META': 'Meta',
            'NFLX': 'Netflix',
            'TSM': 'TSMC',
            'ASML': 'ASML',
            'SMIC': 'SMIC'
        }
        
        return stock_name_mapping.get(code, code)
    
    def _get_keyword_mapping(self) -> Dict[str, List[str]]:
        """í‚¤ì›Œë“œë³„ ê´€ë ¨ ê²€ìƒ‰ì–´ ë§¤í•‘"""
        return {
            'ETF': ['ETF', 'ìƒì¥ì§€ìˆ˜í€ë“œ', 'í€ë“œ'],
            'ë°˜ë„ì²´': ['ë°˜ë„ì²´', 'ì¹©', 'ë©”ëª¨ë¦¬', 'DRAM', 'NAND'],
            '2ì°¨ì „ì§€': ['2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'ë¦¬íŠ¬', 'ì „ê¸°ì°¨'],
            'KOSPI': ['KOSPI', 'ì½”ìŠ¤í”¼', 'ì£¼ê°€', 'ì¦ì‹œ'],
            'ë‚˜ìŠ¤ë‹¥': ['ë‚˜ìŠ¤ë‹¥', 'NASDAQ', 'ë¯¸êµ­', 'í…Œí¬'],
            'ë‹¤ìš°ì¡´ìŠ¤': ['ë‹¤ìš°ì¡´ìŠ¤', 'ë‹¤ìš°', 'DOW', 'ë¯¸êµ­'],
            'S&P500': ['S&P500', 'SP500', 'ë¯¸êµ­', 'ì§€ìˆ˜'],
            'ì›”ê°€': ['ì›”ê°€', 'Wall Street', 'ë‰´ìš•', 'ë¯¸êµ­'],
            'Fed': ['Fed', 'ì—°ì¤€', 'ì—°ë°©ì¤€ë¹„ì œë„', 'ë¯¸êµ­'],
            'ì—°ì¤€': ['ì—°ì¤€', 'Fed', 'ì—°ë°©ì¤€ë¹„ì œë„', 'ë¯¸êµ­']
        }
    
    def _extract_primary_keyword(self, keyword: str) -> str:
        """ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keyword_mapping = self._get_keyword_mapping()
        
        for primary_keyword, related_keywords in keyword_mapping.items():
            if keyword.upper() in [kw.upper() for kw in related_keywords]:
                return primary_keyword
        
        return keyword
    
    def _get_related_keywords(self, keyword: str) -> List[str]:
        """ê´€ë ¨ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
        keyword_mapping = self._get_keyword_mapping()
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        primary_keyword = self._extract_primary_keyword(keyword)
        
        # ê´€ë ¨ í‚¤ì›Œë“œ ë°˜í™˜
        if primary_keyword in keyword_mapping:
            return keyword_mapping[primary_keyword]
        
        # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° ì›ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
        return [keyword]
    
    def _search_naver_news_with_keywords(self, main_keyword: str, related_keywords: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¤‘ë³µ ì œê±° ê°•í™”)"""
        all_news = []
        seen_urls = set()  # URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        seen_headlines = set()  # í—¤ë“œë¼ì¸ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        
        # ë©”ì¸ í‚¤ì›Œë“œë¡œ ë¨¼ì € ê²€ìƒ‰
        main_news = self._search_naver_finance_news(main_keyword)
        if main_news:
            for news in main_news:
                url = news.get('url', '')
                headline = news.get('headline', '').strip()
                
                # URLê³¼ í—¤ë“œë¼ì¸ ëª¨ë‘ ì²´í¬
                if url not in seen_urls and headline not in seen_headlines:
                    all_news.append(news)
                    seen_urls.add(url)
                    seen_headlines.add(headline)
        
        # ê´€ë ¨ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰ (ì¤‘ë³µ ì œê±°)
        for keyword in related_keywords:
            if len(all_news) >= 5:  # ìµœëŒ€ 5ê°œ ë‰´ìŠ¤ë©´ ì¤‘ë‹¨
                break
                
            keyword_news = self._search_naver_finance_news(keyword)
            if keyword_news:
                for news in keyword_news:
                    if len(all_news) >= 5:
                        break
                    
                    url = news.get('url', '')
                    headline = news.get('headline', '').strip()
                    
                    # URLê³¼ í—¤ë“œë¼ì¸ ëª¨ë‘ ì²´í¬í•˜ì—¬ ì¤‘ë³µ ì œê±°
                    if url not in seen_urls and headline not in seen_headlines:
                        all_news.append(news)
                        seen_urls.add(url)
                        seen_headlines.add(headline)
        
        logger.info(f"í‚¤ì›Œë“œ '{main_keyword}'ë¡œ ì¤‘ë³µ ì œê±° í›„ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        return all_news
    
    def _search_naver_news_simple(self, keyword: str) -> List[Dict]:
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¤‘ë³µ ì œê±° ê°•í™”)"""
        if not keyword:
            return []
        
        all_news = []
        seen_urls = set()
        seen_headlines = set()
        
        # 1. ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì‹œë„
        finance_news = self._search_naver_finance_news(keyword)
        if finance_news:
            for news in finance_news:
                url = news.get('url', '')
                headline = news.get('headline', '').strip()
                if url not in seen_urls and headline not in seen_headlines:
                    all_news.append(news)
                    seen_urls.add(url)
                    seen_headlines.add(headline)
        
        # 2. ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ëŒ€ì•ˆ ê²€ìƒ‰ ì‹œë„
        if len(all_news) < 3:  # 3ê°œ ë¯¸ë§Œì´ë©´ ì¶”ê°€ ê²€ìƒ‰
            alt_news = self._search_naver_finance_news_alt(keyword)
            if alt_news:
                for news in alt_news:
                    if len(all_news) >= 5:  # ìµœëŒ€ 5ê°œ
                        break
                    url = news.get('url', '')
                    headline = news.get('headline', '').strip()
                    if url not in seen_urls and headline not in seen_headlines:
                        all_news.append(news)
                        seen_urls.add(url)
                        seen_headlines.add(headline)
        
        # 3. í‚¤ì›Œë“œ ë³€í˜•ìœ¼ë¡œ ì¬ì‹œë„ (ë‰´ìŠ¤ê°€ ë¶€ì¡±í•œ ê²½ìš°)
        if len(all_news) < 2:
            variations = self._get_keyword_variations(keyword)
            for variation in variations:
                if variation != keyword and len(all_news) < 3:
                    news = self._search_naver_finance_news_alt(variation)
                    if news:
                        for item in news:
                            if len(all_news) >= 5:
                                break
                            url = item.get('url', '')
                            headline = item.get('headline', '').strip()
                            if url not in seen_urls and headline not in seen_headlines:
                                all_news.append(item)
                                seen_urls.add(url)
                                seen_headlines.add(headline)
                        if all_news:
                            logger.info(f"í‚¤ì›Œë“œ ë³€í˜• '{variation}'ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ")
                            break
        
        if not all_news:
            logger.warning(f"ëª¨ë“  ë°©ë²•ìœ¼ë¡œ '{keyword}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"'{keyword}' ìµœì¢… ë‰´ìŠ¤ ìˆ˜ì§‘: {len(all_news)}ê°œ")
        return all_news
    
    def _get_keyword_variations(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œ ë³€í˜• ìƒì„± (ì¢…ëª©ë³„ ê³ ìœ í™”)"""
        variations = [keyword]
        
        # í•´ì™¸ ì¢…ëª©ë³„ ê³ ìœ  í‚¤ì›Œë“œ ë§¤í•‘ (ì¤‘ë³µ ë°©ì§€)
        stock_variations = {
            'nvidia': ['ì—”ë¹„ë””ì•„', 'nvidia', 'gpu', 'ai ë°˜ë„ì²´', 'ì—”ë¹„ë””ì•„ ì£¼ê°€'],
            'ì—”ë¹„ë””ì•„': ['nvidia', 'ì—”ë¹„ë””ì•„', 'gpu', 'ai ë°˜ë„ì²´', 'ì—”ë¹„ë””ì•„ ì£¼ê°€'],
            'amd': ['amd', 'amd ì£¼ê°€', 'ë¼ì´ì  ', 'amd ë°˜ë„ì²´'],
            'intel': ['ì¸í…”', 'intel', 'cpu', 'ì¸í…” ì£¼ê°€', 'ì¸í…” ë°˜ë„ì²´'],
            'ì¸í…”': ['intel', 'ì¸í…”', 'cpu', 'ì¸í…” ì£¼ê°€', 'ì¸í…” ë°˜ë„ì²´'], 
            'í€„ì»´': ['qualcomm', 'í€„ì»´', 'ëª¨ë°”ì¼', '5g', 'í€„ì»´ ì£¼ê°€'],
            'ë¸Œë¡œë“œì»´': ['broadcom', 'ë¸Œë¡œë“œì»´', 'ë°˜ë„ì²´', 'ë¸Œë¡œë“œì»´ ì£¼ê°€']
        }
        
        keyword_lower = keyword.lower()
        if keyword_lower in stock_variations:
            # í•´ë‹¹ ì¢…ëª©ì˜ ê³ ìœ  í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            variations.extend(stock_variations[keyword_lower])
        
        # ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ë³€í˜• (ì¢…ëª©ë³„ ê³ ìœ í™”)
        if 'ë°˜ë„ì²´' in keyword and keyword not in ['ì—”ë¹„ë””ì•„', 'nvidia', 'amd', 'intel', 'ì¸í…”', 'í€„ì»´', 'ë¸Œë¡œë“œì»´']:
            variations.extend(['ë°˜ë„ì²´ ì£¼ì‹', 'ë°˜ë„ì²´ ì‹œì¥', 'ai ë°˜ë„ì²´'])
        if 'etf' in keyword.lower():
            variations.extend(['ETF', 'ìƒì¥ì§€ìˆ˜í€ë“œ', 'í€ë“œ'])
        
        return list(set(variations))  # ì¤‘ë³µ ì œê±°
    
    def fetch_naver_news(self, code: str) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ê³¼ ë§í¬ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not code or not isinstance(code, str):
                logger.warning("ìœ íš¨í•˜ì§€ ì•Šì€ ì½”ë“œ ì…ë ¥")
                return []
            
            logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: {code}")
            
            # ì¢…ëª©ëª… ê°€ì ¸ì˜¤ê¸°
            stock_name = self._get_stock_name(code)
            if not stock_name:
                logger.warning(f"ì¢…ëª©ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {code}")
                return []
            
            # í•´ì™¸ ì¢…ëª© ë§¤í•‘ (ì˜ì–´ëª… â†’ í•œêµ­ì–´ëª…)
            overseas_stocks = {
                'NVDA': 'NVIDIA',
                'AMD': 'AMD', 
                'INTC': 'Intel',
                'QCOM': 'Qualcomm',
                'AVGO': 'Broadcom',
                'AAPL': 'Apple',
                'MSFT': 'Microsoft',
                'GOOGL': 'Alphabet',
                'AMZN': 'Amazon',
                'TSLA': 'Tesla',
                'META': 'Meta',
                'NFLX': 'Netflix',
                'TSM': 'TSMC',
                'ASML': 'ASML',
                'SMIC': 'SMIC'
            }
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            search_keywords = [code, stock_name]
            
            # í•´ì™¸ ì¢…ëª©ì¸ ê²½ìš° ì˜ì–´ëª…ê³¼ í•œêµ­ì–´ëª… ëª¨ë‘ ì¶”ê°€
            if code.upper() in overseas_stocks:
                english_name = code.upper()
                korean_name = overseas_stocks[code.upper()]
                search_keywords.extend([english_name, korean_name])
                logger.info(f"í•´ì™¸ ì¢…ëª© ë³€í™˜: {code} â†’ {english_name}, {korean_name}")
            
            # ì¤‘ë³µ ì œê±°
            search_keywords = list(set(search_keywords))
            logger.info(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
            
        
            seen_urls = set()
            seen_headlines = set()
            all_news = []
            
            # 1. ë©”ì¸ í‚¤ì›Œë“œë¡œ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰
            for keyword in search_keywords[:3]:  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                if len(all_news) >= 5:  # ìµœëŒ€ 5ê°œ ë‰´ìŠ¤ë¡œ ì œí•œ
                    break
                    
                logger.info(f"í‚¤ì›Œë“œë¡œ ê¸ˆìœµ ë‰´ìŠ¤ ê²€ìƒ‰: {keyword}")
                finance_news = self._search_naver_finance_news(keyword)
                
                for news in finance_news:
                    if (news['url'] not in seen_urls and 
                        news['headline'] not in seen_headlines and
                        len(all_news) < 5):
                        seen_urls.add(news['url'])
                        seen_headlines.add(news['headline'])
                        all_news.append(news)
            
            # 2. ê¸ˆìœµ ë‰´ìŠ¤ê°€ ë¶€ì¡±í•œ ê²½ìš° ì¼ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰
            if len(all_news) < 3:
                for keyword in search_keywords[:2]:  # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                    if len(all_news) >= 5:
                        break
                        
                    logger.info(f"í‚¤ì›Œë“œë¡œ ì¼ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰: {keyword}")
                    general_news = self._search_naver_general_news(keyword)
                    
                    for news in general_news:
                        if (news['url'] not in seen_urls and 
                            news['headline'] not in seen_headlines and
                            len(all_news) < 5):
                            seen_urls.add(news['url'])
                            seen_headlines.add(news['headline'])
                            all_news.append(news)
            
            # 3. ê´€ë ¨ì„± í•„í„°ë§ (ë§¤ìš° ì—„ê²©í•˜ê²Œ)
            filtered_news = []
            for news in all_news:
                # ë©”ì¸ í‚¤ì›Œë“œì™€ ê´€ë ¨ì„± ì²´í¬
                is_relevant = False
                for keyword in search_keywords[:2]:  # ë©”ì¸ í‚¤ì›Œë“œ 2ê°œë§Œ ì²´í¬
                    if self._is_relevant_news_strict(news['headline'], keyword):
                        is_relevant = True
                        break
                
                # ì¶”ê°€ í•„í„°ë§: ì œì™¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì™„ì „íˆ ì œì™¸
                headline_lower = news['headline'].lower()
                exclude_keywords = ['ì—°ì²´ê¸ˆ', 'ì‹ ìš©ì‚¬ë©´', '324ë§Œëª…', '5000ë§Œì›', 'ë¹š', 'ì—°ë‚´', 'ê°šìœ¼ë©´']
                has_exclude_keyword = any(word in headline_lower for word in exclude_keywords)
                
                # ì¢…ëª©ëª…ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì œì™¸
                has_stock_name = any(keyword.lower() in headline_lower for keyword in search_keywords[:2])
                
                if is_relevant and not has_exclude_keyword and has_stock_name:
                    filtered_news.append(news)
                elif len(filtered_news) < 1 and not has_exclude_keyword and has_stock_name:  # ê´€ë ¨ì„±ì´ ë‚®ì€ ë‰´ìŠ¤ëŠ” ìµœëŒ€ 1ê°œê¹Œì§€ë§Œ
                    filtered_news.append(news)
            
            logger.info(f"ìµœì¢… ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(filtered_news)}ê°œ")
            return filtered_news
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({code}): {e}")
        return []
    
    def analyze_news_sentiment(self, news_items: List[Dict], api_key: str = None) -> List[Dict]:
        """ë‰´ìŠ¤ ê°ì •ë¶„ì„ (GPT í™œìš©)"""
        try:
            # API í‚¤ ì„¤ì •
            if not api_key:
                api_key = os.getenv("OPENAI_API_KEY")
            
            if not api_key:
                logger.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return [{"error": "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}]
            
            client = openai.OpenAI(api_key=api_key)
            
            results = []
            
            for news in news_items:
                headline = news.get('headline', '')
                if not headline:
                    continue
                
                try:
                    # GPT ê°ì •ë¶„ì„ ìš”ì²­
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": SYSTEM_PROMPT_ANALYSIS},
                            {"role": "user", "content": headline}
                        ],
                        max_tokens=100,
                        temperature=0.3
                    )
                    
                    # ì‘ë‹µ íŒŒì‹±
                    analysis_text = response.choices[0].message.content.strip()
                    parts = analysis_text.split('|')
                    
                    if len(parts) >= 3:
                        sentiment_result = {
                            'headline': parts[0].strip(),
                            'sentiment': parts[1].strip(),
                            'reason': parts[2].strip()
                        }
                    else:
                        sentiment_result = {
                            'headline': headline,
                            'sentiment': 'ì¤‘ë¦½',
                            'reason': 'ë¶„ì„ ì‹¤íŒ¨'
                        }
                    
                    results.append(sentiment_result)
                    
                except Exception as e:
                    logger.error(f"ê°ì •ë¶„ì„ ì‹¤íŒ¨ ({headline}): {e}")
                    results.append({
                        'headline': headline,
                        'sentiment': 'ì¤‘ë¦½',
                        'reason': f'ë¶„ì„ ì˜¤ë¥˜: {e}'
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"ê°ì •ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return [{"error": f"ê°ì •ë¶„ì„ ì˜¤ë¥˜: {e}"}]
    
    def generate_level_summary(self, news_items: List[Dict], level: int, api_key: str = None, mpti_type: str = 'Fact') -> str:
        """ë ˆë²¨ë³„ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± (GPT í™œìš©)"""
        try:
            # API í‚¤ ì„¤ì •
            if not api_key:
                api_key = os.getenv("OPENAI_API_KEY")
            
            if not api_key:
                return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            client = openai.OpenAI(api_key=api_key)
            
            # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘
            headlines = [news.get('headline', '') for news in news_items if news.get('headline')]
            
            if not headlines:
                return "ë¶„ì„í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # ë ˆë²¨ë³„ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            level_prompt = config.LEVEL_PROMPTS.get(level, config.LEVEL_PROMPTS[3])
            
            # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
            summary_prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ë“¤ì„ ë¶„ì„í•˜ì—¬ {level_prompt}ì— ë§ëŠ” ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ í—¤ë“œë¼ì¸:
{chr(10).join([f"- {headline}" for headline in headlines])}

ìš”êµ¬ì‚¬í•­:
1. {level_prompt}
2. MPTI íƒ€ì…: {mpti_type}
3. 1-2ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½
4. íˆ¬ì ê´€ì ì—ì„œ ë¶„ì„

ìš”ì•½:
"""
            
            # GPT ìš”ì•½ ìš”ì²­
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íˆ¬ììì—ê²Œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": summary_prompt}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            summary = response.choices[0].message.content.strip()
            return summary
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def display_news_analysis(self, code: str, level: int, mpti_type: str):
        """ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ í‘œì‹œ (Streamlit)"""
        st.subheader("ğŸ“° ë‰´ìŠ¤ ë¶„ì„")
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘
        news_items = self.fetch_naver_news(code)
        
        if not news_items:
            st.warning("ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê°ì •ë¶„ì„
        sentiment_results = self.analyze_news_sentiment(news_items)
        
        # ìš”ì•½ ìƒì„±
        summary = self.generate_level_summary(news_items, level, mpti_type)
        
        # ê²°ê³¼ í‘œì‹œ
        st.write("**ğŸ“Š ë‰´ìŠ¤ ìš”ì•½**")
        st.write(summary)
        
        st.write("**ğŸ“ˆ ê°ì •ë¶„ì„ ê²°ê³¼**")
        for i, result in enumerate(sentiment_results, 1):
            if 'error' in result:
                st.error(result['error'])
            else:
                col1, col2, col3 = st.columns([3, 1, 2])
                with col1:
                    st.write(f"{i}. {result['headline']}")
                with col2:
                    sentiment = result['sentiment']
                    if sentiment == 'ê¸ì •':
                        st.success("ê¸ì •")
                    elif sentiment == 'ë¶€ì •':
                        st.error("ë¶€ì •")
                    else:
                        st.info("ì¤‘ë¦½")
                with col3:
                    st.write(result['reason'])
        
        # ì›ë¬¸ ë§í¬ (í—¤ë“œë¼ì¸ í‘œì‹œ ê°œì„ )
        st.write("**ğŸ”— ê´€ë ¨ ë‰´ìŠ¤**")
        for i, news in enumerate(news_items, 1):
            headline = news.get('headline', '')
            url = news.get('url', '')
            if headline and url:
                st.markdown(f"{i}. **{headline}**")
                st.markdown(f"   [ì›ë¬¸ ë³´ê¸°]({url})")
                st.write("---")

    def test_news_search(self, code: str):
        """ë‰´ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
        print(f"=== ë‰´ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: {code} ===")
        
        # ì¢…ëª©ëª… ê°€ì ¸ì˜¤ê¸°
        stock_name = self._get_stock_name(code)
        print(f"ì¢…ëª©ëª…: {stock_name}")
        
        # ë‰´ìŠ¤ ê²€ìƒ‰
        news_items = self.fetch_naver_news(code)
        print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìˆ˜: {len(news_items)}")
        
        for i, news in enumerate(news_items, 1):
            print(f"{i}. {news['headline']}")
            print(f"   URL: {news['url']}")
            print()
        
        return news_items
